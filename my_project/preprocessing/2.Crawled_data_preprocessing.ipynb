{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.utils import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_crawl1 = pd.read_csv('../private/files/main/crawl193249_194249.csv')\n",
    "df_crawl2 = pd.read_csv('../private/files/main/crawl194249_195249.csv')\n",
    "df_crawl3 = pd.read_csv('../private/files/main/crawl195249_197249.csv')\n",
    "df_crawl4 = pd.read_csv('../private/files/main/crawl197249_198249.csv')\n",
    "df_crawl5 = pd.read_csv('../private/files/main/crawl198249_208249.csv')\n",
    "df_crawl6 = pd.read_csv('../private/files/main/crawl208249_213249.csv')\n",
    "df_crawl7 = pd.read_csv('../private/files/main/crawl213249_223249.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crawl = pd.concat([df_crawl1, df_crawl2, df_crawl3, df_crawl4, df_crawl5, df_crawl6, df_crawl7], axis = 0).drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링 데이터 Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(df_crawl):\n",
    "    df_frozen=pd.read_csv('../private/files/seodam_cleaned_short0307.csv')\n",
    "\n",
    "    df_crawl.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_crawl = df_crawl.ix[df_crawl['title']!='empty']\n",
    "\n",
    "    frozen_list = df_frozen['제목']\n",
    "    df_crawl = df_crawl.ix[~df_crawl['title'].isin(frozen_list)]\n",
    "\n",
    "    df_crawl['text'] = df_crawl['title'].map(str) + ' ' + df_crawl['body']\n",
    "\n",
    "    df_crawl.drop(['title', 'body'], axis=1, inplace=True)\n",
    "\n",
    "    df_crawl.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(df_crawl['text'])):\n",
    "        try:\n",
    "            remove = re.sub(r'[\\n\\r]', ' ', df_crawl.ix[i,'text'])\n",
    "            remove = remove\n",
    "            temp.append(remove)\n",
    "        except:\n",
    "            temp.append('empty')\n",
    "\n",
    "    df_crawl['text2'] = temp\n",
    "\n",
    "    df_crawl = df_crawl.ix[df_crawl['text2']!='empty']\n",
    "\n",
    "    df_crawl.drop('text', axis=1, inplace=True)\n",
    "\n",
    "    df_crawl.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    df_crawl.to_csv('unfrozen193294_223249.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#오류.\n",
    "def preprocessing(df):\n",
    "    frozen = pd.read_csv('./not in use/seodam_cleaned_short0307.csv').drop(['Unnamed: 0'], axis=1)\n",
    "    list_frozen = np.array(frozen['제목'])\n",
    "    \n",
    "    crawl = df.drop(['Unnamed: 0'], axis=1)\n",
    "    crawl = df.ix[crawl['title']!='empty']\n",
    "    crawl2 = df.ix[crawl['title'].isin(list_frozen)]\n",
    "    \n",
    "    return crawl2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
