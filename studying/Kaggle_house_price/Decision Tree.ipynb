{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- null값이 대부분인 변수 제거\n",
    "- 남은 변수의 null값 채우기\n",
    "- skewed data 로그 변환\n",
    "- category 변수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(train, test):\n",
    "    y_train = train.SalePrice\n",
    "    train.drop(['Id','SalePrice'], axis=1, inplace=True)\n",
    "    test.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "    # null 값이 많은 feature 삭제\n",
    "    missing = train.isnull().sum()\n",
    "    to_delete = missing[missing>600]\n",
    "    train.drop(list(to_delete.index), axis=1, inplace=True)\n",
    "    test.drop(list(to_delete.index), axis=1, inplace=True)\n",
    "\n",
    "    category = train.dtypes[train.dtypes=='object'].index\n",
    "    numerical = train.dtypes[train.dtypes!='object'].index\n",
    "\n",
    "    #skewed 된 자료를 log를 취해준다.    \n",
    "    skewness = train[numerical].apply(lambda x : skew(x.dropna()))\n",
    "    skew_idx = skewness[skewness>0.75].index\n",
    "    train[skew_idx]=np.log1p(train[skew_idx])\n",
    "    test[skew_idx]=np.log1p(test[skew_idx])\n",
    "    \n",
    "    #categorical data의 null 값을 최빈값으로 채운다.\n",
    "    for i in category:\n",
    "        train[i].fillna(train[i].mode().values[0], inplace=True)\n",
    "        test[i].fillna(test[i].mode().values[0], inplace=True)\n",
    "\n",
    "    #numerical data의 null 값을 중앙값으로 채운다.\n",
    "    for i in numerical:\n",
    "        train[i].fillna(train[i].median(), inplace=True)\n",
    "        test[i].fillna(test[i].median(), inplace=True)\n",
    "\n",
    "    #categorical 변수 인코딩\n",
    "    train = pd.get_dummies(train)\n",
    "    test = pd.get_dummies(test)\n",
    "    \n",
    "    dif = []\n",
    "    for i in train.columns:\n",
    "        if i not in test.columns:\n",
    "            dif.append(i)\n",
    "\n",
    "    test_null0 = np.zeros((1459,16))\n",
    "    test_null = pd.DataFrame(test_null0, columns=dif)\n",
    "    test = pd.concat([test, test_null], axis=1)    \n",
    "\n",
    "    X_train = train\n",
    "    X_test = test\n",
    "    y_train = np.log1p(y_train)\n",
    "\n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def outlier_detection(X_train,y_train):\n",
    "    If = IsolationForest(contamination=0.05)\n",
    "    If.fit(X_train)\n",
    "    inlier =If.predict(X_train)\n",
    "\n",
    "    outlier_idx = np.where(inlier==-1)[0]\n",
    "\n",
    "    X_train.drop(outlier_idx, axis=0, inplace=True)\n",
    "    y_train.drop(outlier_idx, axis=0, inplace=True)\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 11,\n",
       " 13,\n",
       " 20,\n",
       " 46,\n",
       " 66,\n",
       " 70,\n",
       " 167,\n",
       " 178,\n",
       " 185,\n",
       " 199,\n",
       " 224,\n",
       " 261,\n",
       " 309,\n",
       " 313,\n",
       " 318,\n",
       " 349,\n",
       " 412,\n",
       " 423,\n",
       " 440,\n",
       " 454,\n",
       " 477,\n",
       " 478,\n",
       " 523,\n",
       " 540,\n",
       " 581,\n",
       " 585,\n",
       " 588,\n",
       " 595,\n",
       " 654,\n",
       " 688,\n",
       " 691,\n",
       " 774,\n",
       " 798,\n",
       " 875,\n",
       " 898,\n",
       " 926,\n",
       " 970,\n",
       " 987,\n",
       " 1027,\n",
       " 1109,\n",
       " 1169,\n",
       " 1182,\n",
       " 1239,\n",
       " 1256,\n",
       " 1298,\n",
       " 1324,\n",
       " 1353,\n",
       " 1359,\n",
       " 1405,\n",
       " 1442,\n",
       " 1447]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[4, 11, 13, 20, 46, 66, 70, 167, 178, 185, 199, 224, 261, 309, 313, 318, 349, 412, 423, 440, 454, 477, 478, 523, 540, 581, 585,\n",
    " 588, 595, 654, 688, 691, 774, 798, 875, 898, 926, 970, 987, 1027, 1109, 1169, 1182, 1239, 1256, 1298, 1324, 1353, 1359, 1405, 1442,1447]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(real_value, prediction):\n",
    "    return mean_squared_error(real_value,prediction)**0.5\n",
    "\n",
    "RMSE = make_scorer(error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train):\n",
    "    rfr = RandomForestRegressor(random_state=0)\n",
    "    param_grid = {'n_estimators':[600], 'max_features':[25], 'max_depth':[11]}\n",
    "    model = GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=1, cv=4, scoring=RMSE)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print 'best score:'\n",
    "    print model.best_score_\n",
    "    print\n",
    "    print 'best params:'\n",
    "    print model.best_params_\n",
    "    \n",
    "    return model.best_score_, model.best_params_, model.grid_scores_\n",
    "\n",
    "def gradient_boosting0(X_train, X_test, y_train):\n",
    "    gbr = GradientBoostingRegressor(random_state=0)\n",
    "    param_grid = {'n_estimators': [500],'max_features': [10,15],'max_depth': [6,8,10],'learning_rate': [0.05,0.1,0.15],'subsample': [0.8]}\n",
    "    model = GridSearchCV(estimator=gbr, param_grid=param_grid, n_jobs=1, cv=10, scoring=RMSE)\n",
    "    model.fit(X_train, y_train)\n",
    "    print 'best score:'\n",
    "    print model.best_score_\n",
    "    print\n",
    "    print 'best params:'\n",
    "    print model.best_params_\n",
    "    y_pred = model.predict(X_test)\n",
    "    return np.exp(y_pred)\n",
    "    \n",
    "def gradient_boosting(X_train, X_test, y_train):\n",
    "    gbr = GradientBoostingRegressor(n_estimators=1500, max_features=13, max_depth=3, learning_rate=0.05, random_state=0)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    return np.exp(y_pred)\n",
    "    \n",
    "def extra_tree(X_train, X_test, y_train):\n",
    "    etr=ExtraTreesRegressor(random_state=0)\n",
    "    param_grid = {'n_estimators': [500,600,700], 'max_features': [10,15,20]}\n",
    "    model = GridSearchCV(estimator=etr, param_grid=param_grid, n_jobs=2, cv=4, scoring=RMSE)\n",
    "    model.fit(X_train, y_train)\n",
    "    print 'best score:'\n",
    "    print model.best_score_\n",
    "    print\n",
    "    print 'best params:'\n",
    "    print model.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "Id_list = test.Id.values\n",
    "\n",
    "X_train, X_test, y_train = preprocess(train,test)\n",
    "#X_train, y_train = outlier_detection(X_train0,y_train0)\n",
    "\n",
    "y_pred = gradient_boosting0(X_train,X_test,y_train)\n",
    "result_gbr = pd.DataFrame({'Id':Id_list, 'SalePrice':y_pred})\n",
    "result_gbr.to_csv('./data/result_gbr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
