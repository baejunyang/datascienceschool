{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, TensorFlow\n"
     ]
    }
   ],
   "source": [
    "#상수항 생성\n",
    "hello = tf.constant('hello, TensorFlow')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "#덧셈 노드 생성\n",
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "node3 = node1 + node2\n",
    "\n",
    "sess = tf.Session()\n",
    "print (sess.run([node1, node2]))\n",
    "print (sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "#Feed Dict\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a+b\n",
    "\n",
    "print (sess.run(adder_node, feed_dict={a:3, b:4}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientDecent를 이용한 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.57366 [ 0.84831876] [-1.12570524]\n",
      "20 0.125373 [ 1.320364] [-0.86828363]\n",
      "40 0.0938095 [ 1.34957707] [-0.80800349]\n",
      "60 0.0850174 [ 1.33736336] [-0.76817596]\n",
      "80 0.0772125 [ 1.32191014] [-0.73189801]\n",
      "100 0.0701257 [ 1.30681968] [-0.69748473]\n",
      "120 0.0636893 [ 1.29240394] [-0.66470391]\n",
      "140 0.0578436 [ 1.27866232] [-0.63346499]\n",
      "160 0.0525345 [ 1.26556635] [-0.6036945]\n",
      "180 0.0477127 [ 1.25308561] [-0.57532305]\n",
      "200 0.0433334 [ 1.24119151] [-0.54828489]\n",
      "220 0.0393561 [ 1.22985625] [-0.52251744]\n",
      "240 0.0357438 [ 1.21905375] [-0.49796084]\n",
      "260 0.032463 [ 1.20875907] [-0.47455844]\n",
      "280 0.0294835 [ 1.19894814] [-0.45225596]\n",
      "300 0.0267774 [ 1.18959832] [-0.4310016]\n",
      "320 0.0243196 [ 1.1806879] [-0.41074604]\n",
      "340 0.0220875 [ 1.17219627] [-0.39144254]\n",
      "360 0.0200602 [ 1.16410387] [-0.37304619]\n",
      "380 0.018219 [ 1.15639138] [-0.35551444]\n",
      "400 0.0165468 [ 1.14904141] [-0.33880648]\n",
      "420 0.0150281 [ 1.14203727] [-0.32288393]\n",
      "440 0.0136487 [ 1.13536203] [-0.30770963]\n",
      "460 0.012396 [ 1.12900043] [-0.29324841]\n",
      "480 0.0112582 [ 1.12293816] [-0.27946687]\n",
      "500 0.0102249 [ 1.11716044] [-0.26633301]\n",
      "520 0.00928645 [ 1.11165428] [-0.25381646]\n",
      "540 0.0084341 [ 1.10640693] [-0.24188803]\n",
      "560 0.00765997 [ 1.10140586] [-0.23052008]\n",
      "580 0.0069569 [ 1.09664047] [-0.21968637]\n",
      "600 0.00631837 [ 1.09209871] [-0.20936194]\n",
      "620 0.00573845 [ 1.08777046] [-0.19952275]\n",
      "640 0.00521177 [ 1.08364558] [-0.19014598]\n",
      "660 0.00473339 [ 1.0797143] [-0.18120971]\n",
      "680 0.00429894 [ 1.07596815] [-0.17269346]\n",
      "700 0.00390438 [ 1.07239795] [-0.16457748]\n",
      "720 0.00354601 [ 1.06899548] [-0.15684293]\n",
      "740 0.00322055 [ 1.06575298] [-0.14947191]\n",
      "760 0.00292495 [ 1.06266272] [-0.14244726]\n",
      "780 0.00265647 [ 1.05971766] [-0.13575268]\n",
      "800 0.00241265 [ 1.05691123] [-0.1293727]\n",
      "820 0.00219121 [ 1.05423665] [-0.12329263]\n",
      "840 0.0019901 [ 1.05168784] [-0.11749838]\n",
      "860 0.00180743 [ 1.04925859] [-0.11197644]\n",
      "880 0.00164155 [ 1.04694366] [-0.10671397]\n",
      "900 0.00149088 [ 1.04473746] [-0.10169879]\n",
      "920 0.00135404 [ 1.04263508] [-0.09691937]\n",
      "940 0.00122976 [ 1.04063153] [-0.09236463]\n",
      "960 0.00111689 [ 1.03872168] [-0.0880238]\n",
      "980 0.00101438 [ 1.03690195] [-0.08388697]\n",
      "1000 0.00092127 [ 1.03516769] [-0.07994457]\n",
      "1020 0.000836716 [ 1.03351498] [-0.07618748]\n",
      "1040 0.000759919 [ 1.03193986] [-0.07260692]\n",
      "1060 0.000690166 [ 1.0304389] [-0.06919465]\n",
      "1080 0.00062682 [ 1.02900839] [-0.06594277]\n",
      "1100 0.000569288 [ 1.02764511] [-0.06284372]\n",
      "1120 0.000517038 [ 1.02634585] [-0.05989032]\n",
      "1140 0.000469583 [ 1.02510774] [-0.05707575]\n",
      "1160 0.000426484 [ 1.02392769] [-0.0543934]\n",
      "1180 0.00038734 [ 1.02280331] [-0.05183714]\n",
      "1200 0.000351789 [ 1.02173173] [-0.04940099]\n",
      "1220 0.000319501 [ 1.02071011] [-0.04707941]\n",
      "1240 0.000290173 [ 1.01973653] [-0.04486666]\n",
      "1260 0.000263537 [ 1.0188092] [-0.04275794]\n",
      "1280 0.000239349 [ 1.01792526] [-0.04074842]\n",
      "1300 0.000217381 [ 1.01708281] [-0.03883336]\n",
      "1320 0.000197428 [ 1.01628006] [-0.03700832]\n",
      "1340 0.000179306 [ 1.01551497] [-0.03526908]\n",
      "1360 0.000162849 [ 1.01478577] [-0.03361159]\n",
      "1380 0.000147903 [ 1.0140909] [-0.03203199]\n",
      "1400 0.00013433 [ 1.01342881] [-0.03052667]\n",
      "1420 0.000122 [ 1.01279759] [-0.02909203]\n",
      "1440 0.000110802 [ 1.01219618] [-0.02772479]\n",
      "1460 0.000100632 [ 1.01162302] [-0.02642183]\n",
      "1480 9.13944e-05 [ 1.01107681] [-0.02518013]\n",
      "1500 8.30077e-05 [ 1.01055634] [-0.02399679]\n",
      "1520 7.53893e-05 [ 1.01006031] [-0.02286915]\n",
      "1540 6.8469e-05 [ 1.00958729] [-0.0217944]\n",
      "1560 6.21852e-05 [ 1.0091368] [-0.02077012]\n",
      "1580 5.64784e-05 [ 1.00870752] [-0.01979404]\n",
      "1600 5.12944e-05 [ 1.00829816] [-0.0188638]\n",
      "1620 4.65855e-05 [ 1.00790823] [-0.01797727]\n",
      "1640 4.23104e-05 [ 1.00753665] [-0.01713243]\n",
      "1660 3.84261e-05 [ 1.00718236] [-0.01632728]\n",
      "1680 3.49003e-05 [ 1.006845] [-0.01556]\n",
      "1700 3.16977e-05 [ 1.00652313] [-0.0148288]\n",
      "1720 2.87876e-05 [ 1.00621653] [-0.01413183]\n",
      "1740 2.61453e-05 [ 1.00592446] [-0.01346768]\n",
      "1760 2.3746e-05 [ 1.00564611] [-0.01283479]\n",
      "1780 2.15662e-05 [ 1.00538063] [-0.01223159]\n",
      "1800 1.95872e-05 [ 1.00512791] [-0.01165679]\n",
      "1820 1.77898e-05 [ 1.00488687] [-0.01110903]\n",
      "1840 1.61565e-05 [ 1.00465727] [-0.01058693]\n",
      "1860 1.46738e-05 [ 1.0044384] [-0.01008943]\n",
      "1880 1.33265e-05 [ 1.00422978] [-0.00961526]\n",
      "1900 1.21037e-05 [ 1.00403106] [-0.00916341]\n",
      "1920 1.09928e-05 [ 1.00384152] [-0.00873278]\n",
      "1940 9.9841e-06 [ 1.00366104] [-0.00832237]\n",
      "1960 9.06775e-06 [ 1.0034889] [-0.00793126]\n",
      "1980 8.23509e-06 [ 1.00332499] [-0.00755851]\n",
      "2000 7.47955e-06 [ 1.00316882] [-0.00720333]\n"
     ]
    }
   ],
   "source": [
    "#X_train = [1,2,3]; y_train = [1,2,3]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = X * W + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = \\\n",
    "    sess.run([cost, W, b, train], feed_dict={X:[1,2,3], Y:[1,2,3]})\n",
    "    if step % 20 == 0:\n",
    "        print (step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.00864124  6.01180983]\n"
     ]
    }
   ],
   "source": [
    "print (sess.run(hypothesis, feed_dict={X:[5,6]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W값에 따른 손실함수 값 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFJCAYAAADaPycGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c902e37d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [1,2,3]; Y = [1,2,3]\n",
    "W = tf.placeholder(tf.float32)\n",
    "hypothesis = X*W\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val = []; cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict={W:feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.06261 [ 0.61616522]\n",
      "1 0.586698 [ 0.79528809]\n",
      "2 0.166883 [ 0.89082032]\n",
      "3 0.0474688 [ 0.94177085]\n",
      "4 0.0135023 [ 0.96894443]\n",
      "5 0.00384063 [ 0.98343706]\n",
      "6 0.00109245 [ 0.99116641]\n",
      "7 0.000310748 [ 0.99528873]\n",
      "8 8.83908e-05 [ 0.99748731]\n",
      "9 2.51413e-05 [ 0.99865991]\n",
      "10 7.15128e-06 [ 0.99928528]\n",
      "11 2.03395e-06 [ 0.99961883]\n",
      "12 5.78625e-07 [ 0.99979669]\n",
      "13 1.64532e-07 [ 0.99989158]\n",
      "14 4.68398e-08 [ 0.99994218]\n",
      "15 1.33054e-08 [ 0.99996918]\n",
      "16 3.78884e-09 [ 0.99998355]\n",
      "17 1.07166e-09 [ 0.99999124]\n",
      "18 2.99295e-10 [ 0.99999535]\n",
      "19 8.95426e-11 [ 0.9999975]\n",
      "20 2.31495e-11 [ 0.99999869]\n"
     ]
    }
   ],
   "source": [
    "X_data = [1,2,3]; Y_data=[1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = X*W\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - Y))\n",
    "\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W*X-Y)*X)\n",
    "decent = W - learning_rate*gradient\n",
    "update = W.assign(decent)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={X:X_data, Y:Y_data})\n",
    "    print (step, sess.run(cost, feed_dict={X:X_data, Y:Y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다항회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  98889.3 \n",
      "Prediction:\n",
      " [-133.71398926 -146.32542419 -151.48448181 -165.98878479 -107.237854  ]\n",
      "20 Cost:  21.733 \n",
      "Prediction:\n",
      " [ 145.31494141  188.9788208   178.93170166  193.82263184  148.50091553]\n",
      "40 Cost:  21.5029 \n",
      "Prediction:\n",
      " [ 145.3531189   188.95736694  178.9455719   193.83421326  148.47073364]\n",
      "60 Cost:  21.2752 \n",
      "Prediction:\n",
      " [ 145.38858032  188.93304443  178.95637512  193.84251404  148.43836975]\n",
      "80 Cost:  21.05 \n",
      "Prediction:\n",
      " [ 145.42385864  188.90881348  178.96713257  193.85076904  148.40621948]\n",
      "100 Cost:  20.8273 \n",
      "Prediction:\n",
      " [ 145.4589386   188.88475037  178.97784424  193.85896301  148.3742218 ]\n",
      "120 Cost:  20.6069 \n",
      "Prediction:\n",
      " [ 145.49382019  188.86079407  178.9884491   193.86711121  148.34239197]\n",
      "140 Cost:  20.3889 \n",
      "Prediction:\n",
      " [ 145.52850342  188.83695984  178.99900818  193.87521362  148.31071472]\n",
      "160 Cost:  20.1732 \n",
      "Prediction:\n",
      " [ 145.56300354  188.81324768  179.00950623  193.88328552  148.27920532]\n",
      "180 Cost:  19.9598 \n",
      "Prediction:\n",
      " [ 145.59732056  188.78968811  179.01998901  193.8913269   148.24789429]\n",
      "200 Cost:  19.7488 \n",
      "Prediction:\n",
      " [ 145.63143921  188.76623535  179.03038025  193.89932251  148.21673584]\n",
      "220 Cost:  19.54 \n",
      "Prediction:\n",
      " [ 145.66537476  188.74291992  179.04071045  193.90724182  148.18574524]\n",
      "240 Cost:  19.3335 \n",
      "Prediction:\n",
      " [ 145.6991272   188.71972656  179.05099487  193.91514587  148.15492249]\n",
      "260 Cost:  19.1292 \n",
      "Prediction:\n",
      " [ 145.73271179  188.69667053  179.06121826  193.92303467  148.12428284]\n",
      "280 Cost:  18.9271 \n",
      "Prediction:\n",
      " [ 145.76612854  188.67375183  179.07141113  193.93084717  148.09379578]\n",
      "300 Cost:  18.7273 \n",
      "Prediction:\n",
      " [ 145.79933167  188.65093994  179.08151245  193.93862915  148.06347656]\n",
      "320 Cost:  18.5295 \n",
      "Prediction:\n",
      " [ 145.83233643  188.6282196   179.09155273  193.9463501   148.03327942]\n",
      "340 Cost:  18.3339 \n",
      "Prediction:\n",
      " [ 145.8651886   188.60566711  179.10157776  193.95405579  148.0032959 ]\n",
      "360 Cost:  18.1403 \n",
      "Prediction:\n",
      " [ 145.89787292  188.58322144  179.11152649  193.9617157   147.97343445]\n",
      "380 Cost:  17.9489 \n",
      "Prediction:\n",
      " [ 145.93035889  188.56089783  179.12141418  193.96932983  147.9437561 ]\n",
      "400 Cost:  17.7595 \n",
      "Prediction:\n",
      " [ 145.96269226  188.53871155  179.1312561   193.97692871  147.91426086]\n",
      "420 Cost:  17.5721 \n",
      "Prediction:\n",
      " [ 145.99479675  188.51660156  179.14103699  193.98445129  147.88485718]\n",
      "440 Cost:  17.3869 \n",
      "Prediction:\n",
      " [ 146.02677917  188.49467468  179.15077209  193.99197388  147.85568237]\n",
      "460 Cost:  17.2035 \n",
      "Prediction:\n",
      " [ 146.05856323  188.4728241   179.16046143  193.99943542  147.82661438]\n",
      "480 Cost:  17.0221 \n",
      "Prediction:\n",
      " [ 146.0901947   188.45108032  179.17010498  194.00686646  147.79771423]\n",
      "500 Cost:  16.8427 \n",
      "Prediction:\n",
      " [ 146.12165833  188.42950439  179.17967224  194.01425171  147.76898193]\n",
      "520 Cost:  16.6653 \n",
      "Prediction:\n",
      " [ 146.15292358  188.40800476  179.18917847  194.02159119  147.74040222]\n",
      "540 Cost:  16.4897 \n",
      "Prediction:\n",
      " [ 146.184021    188.3866272   179.19865417  194.02890015  147.71195984]\n",
      "560 Cost:  16.3161 \n",
      "Prediction:\n",
      " [ 146.21495056  188.36535645  179.20805359  194.03616333  147.68367004]\n",
      "580 Cost:  16.1443 \n",
      "Prediction:\n",
      " [ 146.2457428   188.34423828  179.217453    194.04341125  147.6555481 ]\n",
      "600 Cost:  15.9743 \n",
      "Prediction:\n",
      " [ 146.27632141  188.32319641  179.22674561  194.0506134   147.62756348]\n",
      "620 Cost:  15.8062 \n",
      "Prediction:\n",
      " [ 146.3067627   188.30227661  179.23602295  194.05776978  147.59973145]\n",
      "640 Cost:  15.6398 \n",
      "Prediction:\n",
      " [ 146.33705139  188.28149414  179.24525452  194.06491089  147.572052  ]\n",
      "660 Cost:  15.4754 \n",
      "Prediction:\n",
      " [ 146.36714172  188.26081848  179.25440979  194.07199097  147.54452515]\n",
      "680 Cost:  15.3126 \n",
      "Prediction:\n",
      " [ 146.39709473  188.24023438  179.26353455  194.07904053  147.51713562]\n",
      "700 Cost:  15.1516 \n",
      "Prediction:\n",
      " [ 146.42686462  188.2197876   179.27259827  194.08607483  147.48991394]\n",
      "720 Cost:  14.9924 \n",
      "Prediction:\n",
      " [ 146.45646667  188.19941711  179.28158569  194.09303284  147.46279907]\n",
      "740 Cost:  14.8348 \n",
      "Prediction:\n",
      " [ 146.48594666  188.17919922  179.29058838  194.09999084  147.43586731]\n",
      "760 Cost:  14.6789 \n",
      "Prediction:\n",
      " [ 146.51524353  188.15905762  179.29948425  194.10690308  147.40905762]\n",
      "780 Cost:  14.5248 \n",
      "Prediction:\n",
      " [ 146.54437256  188.13903809  179.30834961  194.11378479  147.38241577]\n",
      "800 Cost:  14.3723 \n",
      "Prediction:\n",
      " [ 146.573349    188.11912537  179.31716919  194.12060547  147.355896  ]\n",
      "820 Cost:  14.2214 \n",
      "Prediction:\n",
      " [ 146.60217285  188.09933472  179.32594299  194.12742615  147.32952881]\n",
      "840 Cost:  14.0722 \n",
      "Prediction:\n",
      " [ 146.63082886  188.07963562  179.33467102  194.13418579  147.30329895]\n",
      "860 Cost:  13.9245 \n",
      "Prediction:\n",
      " [ 146.65934753  188.06004333  179.34335327  194.14093018  147.27720642]\n",
      "880 Cost:  13.7785 \n",
      "Prediction:\n",
      " [ 146.68768311  188.04055786  179.35195923  194.14759827  147.25123596]\n",
      "900 Cost:  13.6341 \n",
      "Prediction:\n",
      " [ 146.71588135  188.02119446  179.36054993  194.15426636  147.22544861]\n",
      "920 Cost:  13.4911 \n",
      "Prediction:\n",
      " [ 146.743927    188.00192261  179.36909485  194.16090393  147.19976807]\n",
      "940 Cost:  13.3497 \n",
      "Prediction:\n",
      " [ 146.77183533  187.98274231  179.37757874  194.16749573  147.17424011]\n",
      "960 Cost:  13.2099 \n",
      "Prediction:\n",
      " [ 146.79957581  187.96368408  179.38601685  194.17405701  147.14883423]\n",
      "980 Cost:  13.0715 \n",
      "Prediction:\n",
      " [ 146.82714844  187.94471741  179.39440918  194.18058777  147.12356567]\n",
      "1000 Cost:  12.9346 \n",
      "Prediction:\n",
      " [ 146.854599    187.92585754  179.40275574  194.18708801  147.09843445]\n",
      "1020 Cost:  12.7993 \n",
      "Prediction:\n",
      " [ 146.88188171  187.90711975  179.41105652  194.19354248  147.07344055]\n",
      "1040 Cost:  12.6654 \n",
      "Prediction:\n",
      " [ 146.9090271   187.88845825  179.41931152  194.19995117  147.04859924]\n",
      "1060 Cost:  12.5328 \n",
      "Prediction:\n",
      " [ 146.93603516  187.86990356  179.42753601  194.20635986  147.02386475]\n",
      "1080 Cost:  12.4018 \n",
      "Prediction:\n",
      " [ 146.96286011  187.85145569  179.4356842   194.21270752  146.99926758]\n",
      "1100 Cost:  12.2721 \n",
      "Prediction:\n",
      " [ 146.98957825  187.83311462  179.44381714  194.21905518  146.97480774]\n",
      "1120 Cost:  12.1439 \n",
      "Prediction:\n",
      " [ 147.01612854  187.81486511  179.45188904  194.22535706  146.95048523]\n",
      "1140 Cost:  12.017 \n",
      "Prediction:\n",
      " [ 147.04252625  187.79670715  179.45993042  194.23161316  146.92626953]\n",
      "1160 Cost:  11.8915 \n",
      "Prediction:\n",
      " [ 147.06880188  187.77865601  179.46792603  194.237854    146.90220642]\n",
      "1180 Cost:  11.7673 \n",
      "Prediction:\n",
      " [ 147.09490967  187.76068115  179.4758606   194.24403381  146.87823486]\n",
      "1200 Cost:  11.6445 \n",
      "Prediction:\n",
      " [ 147.12089539  187.74282837  179.48377991  194.25021362  146.85444641]\n",
      "1220 Cost:  11.523 \n",
      "Prediction:\n",
      " [ 147.14675903  187.7250824   179.49163818  194.25637817  146.83078003]\n",
      "1240 Cost:  11.4029 \n",
      "Prediction:\n",
      " [ 147.17242432  187.70741272  179.49943542  194.26243591  146.80718994]\n",
      "1260 Cost:  11.2839 \n",
      "Prediction:\n",
      " [ 147.19801331  187.68984985  179.50721741  194.26852417  146.78375244]\n",
      "1280 Cost:  11.1663 \n",
      "Prediction:\n",
      " [ 147.22341919  187.67237854  179.51495361  194.27456665  146.76045227]\n",
      "1300 Cost:  11.0499 \n",
      "Prediction:\n",
      " [ 147.248703    187.65498352  179.52262878  194.28057861  146.73725891]\n",
      "1320 Cost:  10.9348 \n",
      "Prediction:\n",
      " [ 147.27384949  187.63771057  179.53027344  194.28656006  146.71420288]\n",
      "1340 Cost:  10.821 \n",
      "Prediction:\n",
      " [ 147.29885864  187.62051392  179.53787231  194.29251099  146.69126892]\n",
      "1360 Cost:  10.7084 \n",
      "Prediction:\n",
      " [ 147.32371521  187.60342407  179.54544067  194.2984314   146.66844177]\n",
      "1380 Cost:  10.597 \n",
      "Prediction:\n",
      " [ 147.34844971  187.58641052  179.55296326  194.30432129  146.64576721]\n",
      "1400 Cost:  10.4867 \n",
      "Prediction:\n",
      " [ 147.37307739  187.56951904  179.56044006  194.31018066  146.62319946]\n",
      "1420 Cost:  10.3777 \n",
      "Prediction:\n",
      " [ 147.39752197  187.5526886   179.56787109  194.31600952  146.60073853]\n",
      "1440 Cost:  10.2698 \n",
      "Prediction:\n",
      " [ 147.421875    187.53598022  179.57528687  194.32182312  146.57841492]\n",
      "1460 Cost:  10.1631 \n",
      "Prediction:\n",
      " [ 147.44607544  187.51933289  179.5826416   194.3276062   146.55621338]\n",
      "1480 Cost:  10.0575 \n",
      "Prediction:\n",
      " [ 147.47015381  187.5027771   179.58995056  194.33332825  146.53411865]\n",
      "1500 Cost:  9.95313 \n",
      "Prediction:\n",
      " [ 147.49407959  187.48632812  179.597229    194.33903503  146.512146  ]\n",
      "1520 Cost:  9.84984 \n",
      "Prediction:\n",
      " [ 147.5178833   187.4699707   179.60446167  194.3447113   146.49028015]\n",
      "1540 Cost:  9.74763 \n",
      "Prediction:\n",
      " [ 147.54156494  187.45368958  179.61166382  194.35037231  146.46853638]\n",
      "1560 Cost:  9.64658 \n",
      "Prediction:\n",
      " [ 147.56510925  187.43751526  179.61883545  194.35601807  146.44692993]\n",
      "1580 Cost:  9.54654 \n",
      "Prediction:\n",
      " [ 147.58853149  187.42138672  179.62593079  194.36161804  146.42541504]\n",
      "1600 Cost:  9.4476 \n",
      "Prediction:\n",
      " [ 147.61181641  187.40536499  179.63301086  194.36717224  146.40400696]\n",
      "1620 Cost:  9.34976 \n",
      "Prediction:\n",
      " [ 147.63500977  187.38945007  179.64004517  194.37269592  146.38275146]\n",
      "1640 Cost:  9.253 \n",
      "Prediction:\n",
      " [ 147.65805054  187.37361145  179.64706421  194.3782196   146.36161804]\n",
      "1660 Cost:  9.15722 \n",
      "Prediction:\n",
      " [ 147.68095398  187.35786438  179.65402222  194.38371277  146.34054565]\n",
      "1680 Cost:  9.06248 \n",
      "Prediction:\n",
      " [ 147.70373535  187.34217834  179.66091919  194.3891449   146.31959534]\n",
      "1700 Cost:  8.96874 \n",
      "Prediction:\n",
      " [ 147.72640991  187.32659912  179.66783142  194.39459229  146.29876709]\n",
      "1720 Cost:  8.87604 \n",
      "Prediction:\n",
      " [ 147.7489624   187.31111145  179.67469788  194.3999939   146.27806091]\n",
      "1740 Cost:  8.78436 \n",
      "Prediction:\n",
      " [ 147.7713623   187.29570007  179.68148804  194.40536499  146.25744629]\n",
      "1760 Cost:  8.69363 \n",
      "Prediction:\n",
      " [ 147.79367065  187.28036499  179.68826294  194.41072083  146.23696899]\n",
      "1780 Cost:  8.60388 \n",
      "Prediction:\n",
      " [ 147.81585693  187.26512146  179.69502258  194.41601562  146.21658325]\n",
      "1800 Cost:  8.51512 \n",
      "Prediction:\n",
      " [ 147.83790588  187.24995422  179.70172119  194.42132568  146.19631958]\n",
      "1820 Cost:  8.42729 \n",
      "Prediction:\n",
      " [ 147.8598175   187.23484802  179.70835876  194.42657471  146.1761322 ]\n",
      "1840 Cost:  8.34039 \n",
      "Prediction:\n",
      " [ 147.88165283  187.21986389  179.71499634  194.43183899  146.15608215]\n",
      "1860 Cost:  8.25449 \n",
      "Prediction:\n",
      " [ 147.90335083  187.20497131  179.72158813  194.43704224  146.13613892]\n",
      "1880 Cost:  8.16945 \n",
      "Prediction:\n",
      " [ 147.92492676  187.19012451  179.72813416  194.44223022  146.11628723]\n",
      "1900 Cost:  8.08533 \n",
      "Prediction:\n",
      " [ 147.94639587  187.175354    179.73466492  194.4473877   146.09655762]\n",
      "1920 Cost:  8.00213 \n",
      "Prediction:\n",
      " [ 147.96772766  187.16069031  179.7411499   194.45251465  146.0769043 ]\n",
      "1940 Cost:  7.91983 \n",
      "Prediction:\n",
      " [ 147.98895264  187.14607239  179.74757385  194.45762634  146.05738831]\n",
      "1960 Cost:  7.83838 \n",
      "Prediction:\n",
      " [ 148.01008606  187.13156128  179.7539978   194.46270752  146.03796387]\n",
      "1980 Cost:  7.75785 \n",
      "Prediction:\n",
      " [ 148.03108215  187.11712646  179.76037598  194.46775818  146.01864624]\n",
      "2000 Cost:  7.67822 \n",
      "Prediction:\n",
      " [ 148.05194092  187.10276794  179.76672363  194.47277832  145.99943542]\n"
     ]
    }
   ],
   "source": [
    "x1_data = [73.,93.,89.,96.,73.]; x2_data = [80.,88.,91.,98.,66.]; x3_data=[75.,93.,90.,100.,70.]\n",
    "y_data=[152.,185.,180.,196.,142.]\n",
    "\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x1*w1 + x2*w2 + x3*w3 + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost,hypothesis, train],\n",
    "                        feed_dict={x1:x1_data, x2:x2_data, x3:x3_data, Y:y_data})\n",
    "    if step%20 == 0:\n",
    "        print(step, \"Cost: \",cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬을 사용한 다항회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for step in range(2001):\\n    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\\n                        feed_dict={X:x_data, Y:y_data})\\n    if step%20 == 0:\\n        print (step, \"Cost: \",cost_val, \"\\nPrediction\\n:\", hy_val)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_data = [73.,93.,89.,96.,73.]; x2_data = [80.,88.,91.,98.,66.]; x3_data=[75.,93.,90.,100.,70.]\n",
    "x_data = zip(x1_data, x2_data, x3_data)\n",
    "y_data=np.array([152.,185.,180.,196.,142.])\n",
    "\n",
    "x_data = [list(i) for i in x_data]\n",
    "y_data = y_data[:,np.newaxis]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "\n",
    "hypothesis = tf.matmul(X,W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "'''for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                        feed_dict={X:x_data, Y:y_data})\n",
    "    if step%20 == 0:\n",
    "        print (step, \"Cost: \",cost_val, \"\\nPrediction\\n:\", hy_val)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 단위로 파일을 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  51.3228 \n",
      "Prediction:\n",
      " [[ 143.84683228]\n",
      " [ 180.25315857]\n",
      " [ 173.76638794]\n",
      " [ 189.35366821]\n",
      " [ 139.29452515]\n",
      " [ 103.88547516]\n",
      " [ 139.95726013]\n",
      " [ 101.01110077]\n",
      " [ 170.3841095 ]\n",
      " [ 158.82528687]]\n",
      "20 Cost:  19.4212 \n",
      "Prediction:\n",
      " [[ 150.804245  ]\n",
      " [ 188.58354187]\n",
      " [ 181.99032593]\n",
      " [ 198.31637573]\n",
      " [ 145.63476562]\n",
      " [ 108.56574249]\n",
      " [ 146.68429565]\n",
      " [ 106.02444458]\n",
      " [ 178.16964722]\n",
      " [ 166.04745483]]\n",
      "40 Cost:  19.2689 \n",
      "Prediction:\n",
      " [[ 150.81733704]\n",
      " [ 188.56629944]\n",
      " [ 181.98985291]\n",
      " [ 198.32176208]\n",
      " [ 145.60856628]\n",
      " [ 108.55166626]\n",
      " [ 146.7063446 ]\n",
      " [ 106.06697083]\n",
      " [ 178.15310669]\n",
      " [ 166.04222107]]\n",
      "60 Cost:  19.1173 \n",
      "Prediction:\n",
      " [[ 150.83004761]\n",
      " [ 188.54876709]\n",
      " [ 181.98901367]\n",
      " [ 198.32670593]\n",
      " [ 145.58218384]\n",
      " [ 108.53744507]\n",
      " [ 146.72799683]\n",
      " [ 106.10905457]\n",
      " [ 178.13627625]\n",
      " [ 166.03666687]]\n",
      "80 Cost:  18.9671 \n",
      "Prediction:\n",
      " [[ 150.84272766]\n",
      " [ 188.53132629]\n",
      " [ 181.98817444]\n",
      " [ 198.33161926]\n",
      " [ 145.55595398]\n",
      " [ 108.52327728]\n",
      " [ 146.74952698]\n",
      " [ 106.15093994]\n",
      " [ 178.11952209]\n",
      " [ 166.03111267]]\n",
      "100 Cost:  18.8184 \n",
      "Prediction:\n",
      " [[ 150.85531616]\n",
      " [ 188.51397705]\n",
      " [ 181.98736572]\n",
      " [ 198.33653259]\n",
      " [ 145.52984619]\n",
      " [ 108.50918579]\n",
      " [ 146.77096558]\n",
      " [ 106.19261932]\n",
      " [ 178.10284424]\n",
      " [ 166.02560425]]\n",
      "120 Cost:  18.6712 \n",
      "Prediction:\n",
      " [[ 150.86787415]\n",
      " [ 188.49668884]\n",
      " [ 181.98652649]\n",
      " [ 198.34138489]\n",
      " [ 145.50386047]\n",
      " [ 108.49514771]\n",
      " [ 146.79225159]\n",
      " [ 106.23406982]\n",
      " [ 178.08622742]\n",
      " [ 166.02008057]]\n",
      "140 Cost:  18.5256 \n",
      "Prediction:\n",
      " [[ 150.88037109]\n",
      " [ 188.47950745]\n",
      " [ 181.98571777]\n",
      " [ 198.34625244]\n",
      " [ 145.47801208]\n",
      " [ 108.48118591]\n",
      " [ 146.81347656]\n",
      " [ 106.27532196]\n",
      " [ 178.06971741]\n",
      " [ 166.0145874 ]]\n",
      "160 Cost:  18.3814 \n",
      "Prediction:\n",
      " [[ 150.89279175]\n",
      " [ 188.46238708]\n",
      " [ 181.98490906]\n",
      " [ 198.35107422]\n",
      " [ 145.45228577]\n",
      " [ 108.46727753]\n",
      " [ 146.83457947]\n",
      " [ 106.31636047]\n",
      " [ 178.05326843]\n",
      " [ 166.0091095 ]]\n",
      "180 Cost:  18.2386 \n",
      "Prediction:\n",
      " [[ 150.90518188]\n",
      " [ 188.44538879]\n",
      " [ 181.9841156 ]\n",
      " [ 198.35588074]\n",
      " [ 145.42668152]\n",
      " [ 108.45343781]\n",
      " [ 146.85557556]\n",
      " [ 106.35718536]\n",
      " [ 178.03691101]\n",
      " [ 166.00364685]]\n",
      "200 Cost:  18.0973 \n",
      "Prediction:\n",
      " [[ 150.91749573]\n",
      " [ 188.42843628]\n",
      " [ 181.98330688]\n",
      " [ 198.36065674]\n",
      " [ 145.4012146 ]\n",
      " [ 108.43965912]\n",
      " [ 146.87646484]\n",
      " [ 106.39781189]\n",
      " [ 178.02059937]\n",
      " [ 165.99819946]]\n",
      "220 Cost:  17.9574 \n",
      "Prediction:\n",
      " [[ 150.92974854]\n",
      " [ 188.41157532]\n",
      " [ 181.98252869]\n",
      " [ 198.36540222]\n",
      " [ 145.37585449]\n",
      " [ 108.42594147]\n",
      " [ 146.89724731]\n",
      " [ 106.43822479]\n",
      " [ 178.00437927]\n",
      " [ 165.99275208]]\n",
      "240 Cost:  17.8189 \n",
      "Prediction:\n",
      " [[ 150.94195557]\n",
      " [ 188.39479065]\n",
      " [ 181.98173523]\n",
      " [ 198.37014771]\n",
      " [ 145.35064697]\n",
      " [ 108.41228485]\n",
      " [ 146.91792297]\n",
      " [ 106.47844696]\n",
      " [ 177.98822021]\n",
      " [ 165.98733521]]\n",
      "260 Cost:  17.6818 \n",
      "Prediction:\n",
      " [[ 150.95410156]\n",
      " [ 188.37809753]\n",
      " [ 181.98097229]\n",
      " [ 198.37486267]\n",
      " [ 145.32553101]\n",
      " [ 108.3986969 ]\n",
      " [ 146.93849182]\n",
      " [ 106.51846313]\n",
      " [ 177.97215271]\n",
      " [ 165.98193359]]\n",
      "280 Cost:  17.546 \n",
      "Prediction:\n",
      " [[ 150.96618652]\n",
      " [ 188.36148071]\n",
      " [ 181.98019409]\n",
      " [ 198.37953186]\n",
      " [ 145.30056763]\n",
      " [ 108.38516998]\n",
      " [ 146.95895386]\n",
      " [ 106.55826569]\n",
      " [ 177.95614624]\n",
      " [ 165.97653198]]\n",
      "300 Cost:  17.4116 \n",
      "Prediction:\n",
      " [[ 150.97822571]\n",
      " [ 188.34492493]\n",
      " [ 181.97941589]\n",
      " [ 198.38418579]\n",
      " [ 145.2756958 ]\n",
      " [ 108.37169647]\n",
      " [ 146.97932434]\n",
      " [ 106.5978775 ]\n",
      " [ 177.94020081]\n",
      " [ 165.97116089]]\n",
      "320 Cost:  17.2786 \n",
      "Prediction:\n",
      " [[ 150.99020386]\n",
      " [ 188.32849121]\n",
      " [ 181.9786377 ]\n",
      " [ 198.38882446]\n",
      " [ 145.25097656]\n",
      " [ 108.35829926]\n",
      " [ 146.99958801]\n",
      " [ 106.63729095]\n",
      " [ 177.92436218]\n",
      " [ 165.96580505]]\n",
      "340 Cost:  17.1469 \n",
      "Prediction:\n",
      " [[ 151.00212097]\n",
      " [ 188.31210327]\n",
      " [ 181.97789001]\n",
      " [ 198.39343262]\n",
      " [ 145.22636414]\n",
      " [ 108.34494781]\n",
      " [ 147.01974487]\n",
      " [ 106.67650604]\n",
      " [ 177.90858459]\n",
      " [ 165.96046448]]\n",
      "360 Cost:  17.0166 \n",
      "Prediction:\n",
      " [[ 151.01400757]\n",
      " [ 188.29582214]\n",
      " [ 181.97714233]\n",
      " [ 198.39804077]\n",
      " [ 145.20187378]\n",
      " [ 108.33167267]\n",
      " [ 147.03982544]\n",
      " [ 106.71552277]\n",
      " [ 177.8928833 ]\n",
      " [ 165.95513916]]\n",
      "380 Cost:  16.8875 \n",
      "Prediction:\n",
      " [[ 151.02580261]\n",
      " [ 188.27960205]\n",
      " [ 181.97639465]\n",
      " [ 198.40258789]\n",
      " [ 145.17749023]\n",
      " [ 108.31845856]\n",
      " [ 147.05975342]\n",
      " [ 106.75434113]\n",
      " [ 177.87722778]\n",
      " [ 165.94981384]]\n",
      "400 Cost:  16.7598 \n",
      "Prediction:\n",
      " [[ 151.03755188]\n",
      " [ 188.26345825]\n",
      " [ 181.97566223]\n",
      " [ 198.40713501]\n",
      " [ 145.15324402]\n",
      " [ 108.30529785]\n",
      " [ 147.07962036]\n",
      " [ 106.79296112]\n",
      " [ 177.86166382]\n",
      " [ 165.94450378]]\n",
      "420 Cost:  16.6333 \n",
      "Prediction:\n",
      " [[ 151.04925537]\n",
      " [ 188.24737549]\n",
      " [ 181.97491455]\n",
      " [ 198.41165161]\n",
      " [ 145.12910461]\n",
      " [ 108.29218292]\n",
      " [ 147.09936523]\n",
      " [ 106.83138275]\n",
      " [ 177.84616089]\n",
      " [ 165.9392395 ]]\n",
      "440 Cost:  16.5081 \n",
      "Prediction:\n",
      " [[ 151.06091309]\n",
      " [ 188.23141479]\n",
      " [ 181.97419739]\n",
      " [ 198.41615295]\n",
      " [ 145.10510254]\n",
      " [ 108.27915192]\n",
      " [ 147.11903381]\n",
      " [ 106.86962128]\n",
      " [ 177.83073425]\n",
      " [ 165.93397522]]\n",
      "460 Cost:  16.3842 \n",
      "Prediction:\n",
      " [[ 151.07249451]\n",
      " [ 188.21549988]\n",
      " [ 181.97346497]\n",
      " [ 198.42062378]\n",
      " [ 145.08120728]\n",
      " [ 108.26615906]\n",
      " [ 147.13858032]\n",
      " [ 106.90763855]\n",
      " [ 177.81538391]\n",
      " [ 165.92871094]]\n",
      "480 Cost:  16.2615 \n",
      "Prediction:\n",
      " [[ 151.08404541]\n",
      " [ 188.19967651]\n",
      " [ 181.9727478 ]\n",
      " [ 198.42506409]\n",
      " [ 145.05743408]\n",
      " [ 108.25325012]\n",
      " [ 147.15803528]\n",
      " [ 106.94549561]\n",
      " [ 177.8000946 ]\n",
      " [ 165.92347717]]\n",
      "500 Cost:  16.1401 \n",
      "Prediction:\n",
      " [[ 151.09553528]\n",
      " [ 188.18392944]\n",
      " [ 181.97203064]\n",
      " [ 198.42950439]\n",
      " [ 145.0337677 ]\n",
      " [ 108.24037933]\n",
      " [ 147.17739868]\n",
      " [ 106.9831543 ]\n",
      " [ 177.78488159]\n",
      " [ 165.91824341]]\n",
      "520 Cost:  16.0199 \n",
      "Prediction:\n",
      " [[ 151.10697937]\n",
      " [ 188.16825867]\n",
      " [ 181.97132874]\n",
      " [ 198.43391418]\n",
      " [ 145.01023865]\n",
      " [ 108.22758484]\n",
      " [ 147.19665527]\n",
      " [ 107.02061462]\n",
      " [ 177.76974487]\n",
      " [ 165.91307068]]\n",
      "540 Cost:  15.9008 \n",
      "Prediction:\n",
      " [[ 151.11834717]\n",
      " [ 188.15264893]\n",
      " [ 181.97061157]\n",
      " [ 198.43829346]\n",
      " [ 144.98680115]\n",
      " [ 108.21482849]\n",
      " [ 147.21582031]\n",
      " [ 107.05788422]\n",
      " [ 177.75466919]\n",
      " [ 165.90786743]]\n",
      "560 Cost:  15.783 \n",
      "Prediction:\n",
      " [[ 151.12968445]\n",
      " [ 188.13713074]\n",
      " [ 181.96992493]\n",
      " [ 198.44265747]\n",
      " [ 144.96350098]\n",
      " [ 108.20214081]\n",
      " [ 147.2348938 ]\n",
      " [ 107.09497833]\n",
      " [ 177.7396698 ]\n",
      " [ 165.9026947 ]]\n",
      "580 Cost:  15.6664 \n",
      "Prediction:\n",
      " [[ 151.14096069]\n",
      " [ 188.12168884]\n",
      " [ 181.96923828]\n",
      " [ 198.44699097]\n",
      " [ 144.94029236]\n",
      " [ 108.18952179]\n",
      " [ 147.25386047]\n",
      " [ 107.13187408]\n",
      " [ 177.72473145]\n",
      " [ 165.89753723]]\n",
      "600 Cost:  15.551 \n",
      "Prediction:\n",
      " [[ 151.15216064]\n",
      " [ 188.10630798]\n",
      " [ 181.96853638]\n",
      " [ 198.45129395]\n",
      " [ 144.91720581]\n",
      " [ 108.17695618]\n",
      " [ 147.27272034]\n",
      " [ 107.16858673]\n",
      " [ 177.70986938]\n",
      " [ 165.89237976]]\n",
      "620 Cost:  15.4367 \n",
      "Prediction:\n",
      " [[ 151.16334534]\n",
      " [ 188.09101868]\n",
      " [ 181.96784973]\n",
      " [ 198.45561218]\n",
      " [ 144.89424133]\n",
      " [ 108.1644516 ]\n",
      " [ 147.29150391]\n",
      " [ 107.20513153]\n",
      " [ 177.69506836]\n",
      " [ 165.88726807]]\n",
      "640 Cost:  15.3236 \n",
      "Prediction:\n",
      " [[ 151.17445374]\n",
      " [ 188.07580566]\n",
      " [ 181.96717834]\n",
      " [ 198.45986938]\n",
      " [ 144.87138367]\n",
      " [ 108.15197754]\n",
      " [ 147.31018066]\n",
      " [ 107.24146271]\n",
      " [ 177.68032837]\n",
      " [ 165.88214111]]\n",
      "660 Cost:  15.2116 \n",
      "Prediction:\n",
      " [[ 151.18553162]\n",
      " [ 188.06063843]\n",
      " [ 181.9664917 ]\n",
      " [ 198.46412659]\n",
      " [ 144.84863281]\n",
      " [ 108.1395874 ]\n",
      " [ 147.32878113]\n",
      " [ 107.27761841]\n",
      " [ 177.66566467]\n",
      " [ 165.87705994]]\n",
      "680 Cost:  15.1007 \n",
      "Prediction:\n",
      " [[ 151.19654846]\n",
      " [ 188.045578  ]\n",
      " [ 181.96583557]\n",
      " [ 198.46838379]\n",
      " [ 144.82601929]\n",
      " [ 108.12725067]\n",
      " [ 147.34727478]\n",
      " [ 107.31360626]\n",
      " [ 177.65109253]\n",
      " [ 165.87197876]]\n",
      "700 Cost:  14.991 \n",
      "Prediction:\n",
      " [[ 151.20750427]\n",
      " [ 188.03056335]\n",
      " [ 181.96516418]\n",
      " [ 198.47257996]\n",
      " [ 144.80348206]\n",
      " [ 108.11495972]\n",
      " [ 147.36567688]\n",
      " [ 107.34938812]\n",
      " [ 177.6365509 ]\n",
      " [ 165.86691284]]\n",
      "720 Cost:  14.8823 \n",
      "Prediction:\n",
      " [[ 151.21841431]\n",
      " [ 188.01564026]\n",
      " [ 181.96450806]\n",
      " [ 198.47677612]\n",
      " [ 144.78108215]\n",
      " [ 108.10273743]\n",
      " [ 147.38400269]\n",
      " [ 107.38500977]\n",
      " [ 177.62210083]\n",
      " [ 165.86187744]]\n",
      "740 Cost:  14.7748 \n",
      "Prediction:\n",
      " [[ 151.22927856]\n",
      " [ 188.00079346]\n",
      " [ 181.96385193]\n",
      " [ 198.48092651]\n",
      " [ 144.75878906]\n",
      " [ 108.09057617]\n",
      " [ 147.40220642]\n",
      " [ 107.4204483 ]\n",
      " [ 177.60769653]\n",
      " [ 165.85682678]]\n",
      "760 Cost:  14.6683 \n",
      "Prediction:\n",
      " [[ 151.24006653]\n",
      " [ 187.98597717]\n",
      " [ 181.96318054]\n",
      " [ 198.4850769 ]\n",
      " [ 144.73658752]\n",
      " [ 108.07844543]\n",
      " [ 147.42033386]\n",
      " [ 107.45569611]\n",
      " [ 177.59336853]\n",
      " [ 165.85179138]]\n",
      "780 Cost:  14.563 \n",
      "Prediction:\n",
      " [[ 151.25083923]\n",
      " [ 187.97128296]\n",
      " [ 181.96255493]\n",
      " [ 198.48921204]\n",
      " [ 144.71450806]\n",
      " [ 108.06639099]\n",
      " [ 147.43835449]\n",
      " [ 107.49078369]\n",
      " [ 177.57910156]\n",
      " [ 165.84680176]]\n",
      "800 Cost:  14.4587 \n",
      "Prediction:\n",
      " [[ 151.2615509 ]\n",
      " [ 187.95664978]\n",
      " [ 181.96192932]\n",
      " [ 198.49333191]\n",
      " [ 144.69255066]\n",
      " [ 108.05438995]\n",
      " [ 147.45631409]\n",
      " [ 107.52568817]\n",
      " [ 177.56492615]\n",
      " [ 165.84181213]]\n",
      "820 Cost:  14.3554 \n",
      "Prediction:\n",
      " [[ 151.2722168 ]\n",
      " [ 187.94207764]\n",
      " [ 181.96128845]\n",
      " [ 198.49740601]\n",
      " [ 144.67066956]\n",
      " [ 108.04244232]\n",
      " [ 147.47415161]\n",
      " [ 107.56040955]\n",
      " [ 177.55079651]\n",
      " [ 165.83685303]]\n",
      "840 Cost:  14.2531 \n",
      "Prediction:\n",
      " [[ 151.28282166]\n",
      " [ 187.92756653]\n",
      " [ 181.96064758]\n",
      " [ 198.50146484]\n",
      " [ 144.64891052]\n",
      " [ 108.03055573]\n",
      " [ 147.49191284]\n",
      " [ 107.59496307]\n",
      " [ 177.53671265]\n",
      " [ 165.83187866]]\n",
      "860 Cost:  14.152 \n",
      "Prediction:\n",
      " [[ 151.29338074]\n",
      " [ 187.91313171]\n",
      " [ 181.96000671]\n",
      " [ 198.50550842]\n",
      " [ 144.6272583 ]\n",
      " [ 108.0187149 ]\n",
      " [ 147.50958252]\n",
      " [ 107.6293335 ]\n",
      " [ 177.52272034]\n",
      " [ 165.82695007]]\n",
      "880 Cost:  14.0518 \n",
      "Prediction:\n",
      " [[ 151.30389404]\n",
      " [ 187.89878845]\n",
      " [ 181.95941162]\n",
      " [ 198.50952148]\n",
      " [ 144.60571289]\n",
      " [ 108.00693512]\n",
      " [ 147.5271759 ]\n",
      " [ 107.6635437 ]\n",
      " [ 177.50878906]\n",
      " [ 165.82202148]]\n",
      "900 Cost:  13.9527 \n",
      "Prediction:\n",
      " [[ 151.31436157]\n",
      " [ 187.88450623]\n",
      " [ 181.95878601]\n",
      " [ 198.51353455]\n",
      " [ 144.58430481]\n",
      " [ 107.99520111]\n",
      " [ 147.54466248]\n",
      " [ 107.69757843]\n",
      " [ 177.49491882]\n",
      " [ 165.81712341]]\n",
      "920 Cost:  13.8545 \n",
      "Prediction:\n",
      " [[ 151.32476807]\n",
      " [ 187.87028503]\n",
      " [ 181.95817566]\n",
      " [ 198.51751709]\n",
      " [ 144.56295776]\n",
      " [ 107.9835434 ]\n",
      " [ 147.5620575 ]\n",
      " [ 107.73143768]\n",
      " [ 177.48112488]\n",
      " [ 165.81222534]]\n",
      "940 Cost:  13.7573 \n",
      "Prediction:\n",
      " [[ 151.33511353]\n",
      " [ 187.85612488]\n",
      " [ 181.95755005]\n",
      " [ 198.52146912]\n",
      " [ 144.54171753]\n",
      " [ 107.97190857]\n",
      " [ 147.57937622]\n",
      " [ 107.7651062 ]\n",
      " [ 177.46737671]\n",
      " [ 165.80734253]]\n",
      "960 Cost:  13.6612 \n",
      "Prediction:\n",
      " [[ 151.34544373]\n",
      " [ 187.84205627]\n",
      " [ 181.95695496]\n",
      " [ 198.52542114]\n",
      " [ 144.52061462]\n",
      " [ 107.96034241]\n",
      " [ 147.59660339]\n",
      " [ 107.79862976]\n",
      " [ 177.45368958]\n",
      " [ 165.80247498]]\n",
      "980 Cost:  13.566 \n",
      "Prediction:\n",
      " [[ 151.35571289]\n",
      " [ 187.82803345]\n",
      " [ 181.95635986]\n",
      " [ 198.52935791]\n",
      " [ 144.49958801]\n",
      " [ 107.94882965]\n",
      " [ 147.61376953]\n",
      " [ 107.83197784]\n",
      " [ 177.44009399]\n",
      " [ 165.79763794]]\n",
      "1000 Cost:  13.4717 \n",
      "Prediction:\n",
      " [[ 151.36592102]\n",
      " [ 187.81408691]\n",
      " [ 181.95574951]\n",
      " [ 198.53326416]\n",
      " [ 144.47866821]\n",
      " [ 107.9373703 ]\n",
      " [ 147.63079834]\n",
      " [ 107.86517334]\n",
      " [ 177.42654419]\n",
      " [ 165.79281616]]\n",
      "1020 Cost:  13.3784 \n",
      "Prediction:\n",
      " [[ 151.37608337]\n",
      " [ 187.80021667]\n",
      " [ 181.95515442]\n",
      " [ 198.53712463]\n",
      " [ 144.45785522]\n",
      " [ 107.92596436]\n",
      " [ 147.64778137]\n",
      " [ 107.8981781 ]\n",
      " [ 177.41304016]\n",
      " [ 165.78799438]]\n",
      "1040 Cost:  13.2861 \n",
      "Prediction:\n",
      " [[ 151.38623047]\n",
      " [ 187.78642273]\n",
      " [ 181.95457458]\n",
      " [ 198.54100037]\n",
      " [ 144.43716431]\n",
      " [ 107.91461945]\n",
      " [ 147.66467285]\n",
      " [ 107.93103027]\n",
      " [ 177.39962769]\n",
      " [ 165.78320312]]\n",
      "1060 Cost:  13.1947 \n",
      "Prediction:\n",
      " [[ 151.39628601]\n",
      " [ 187.77267456]\n",
      " [ 181.95399475]\n",
      " [ 198.54483032]\n",
      " [ 144.41654968]\n",
      " [ 107.90331268]\n",
      " [ 147.68145752]\n",
      " [ 107.96370697]\n",
      " [ 177.38626099]\n",
      " [ 165.77841187]]\n",
      "1080 Cost:  13.1042 \n",
      "Prediction:\n",
      " [[ 151.40631104]\n",
      " [ 187.75898743]\n",
      " [ 181.95341492]\n",
      " [ 198.54866028]\n",
      " [ 144.39604187]\n",
      " [ 107.89205933]\n",
      " [ 147.69818115]\n",
      " [ 107.99621582]\n",
      " [ 177.37297058]\n",
      " [ 165.77363586]]\n",
      "1100 Cost:  13.0146 \n",
      "Prediction:\n",
      " [[ 151.41629028]\n",
      " [ 187.74539185]\n",
      " [ 181.95283508]\n",
      " [ 198.55247498]\n",
      " [ 144.37564087]\n",
      " [ 107.880867  ]\n",
      " [ 147.71479797]\n",
      " [ 108.02856445]\n",
      " [ 177.35971069]\n",
      " [ 165.76889038]]\n",
      "1120 Cost:  12.926 \n",
      "Prediction:\n",
      " [[ 151.42623901]\n",
      " [ 187.7318573 ]\n",
      " [ 181.95225525]\n",
      " [ 198.55625916]\n",
      " [ 144.35533142]\n",
      " [ 107.86971283]\n",
      " [ 147.73135376]\n",
      " [ 108.06075287]\n",
      " [ 177.34654236]\n",
      " [ 165.76416016]]\n",
      "1140 Cost:  12.8382 \n",
      "Prediction:\n",
      " [[ 151.43612671]\n",
      " [ 187.71836853]\n",
      " [ 181.95169067]\n",
      " [ 198.56002808]\n",
      " [ 144.33512878]\n",
      " [ 107.85862732]\n",
      " [ 147.74781799]\n",
      " [ 108.0927887 ]\n",
      " [ 177.3334198 ]\n",
      " [ 165.75944519]]\n",
      "1160 Cost:  12.7514 \n",
      "Prediction:\n",
      " [[ 151.44596863]\n",
      " [ 187.70497131]\n",
      " [ 181.95114136]\n",
      " [ 198.56378174]\n",
      " [ 144.31503296]\n",
      " [ 107.84759521]\n",
      " [ 147.76419067]\n",
      " [ 108.12464905]\n",
      " [ 177.32037354]\n",
      " [ 165.75473022]]\n",
      "1180 Cost:  12.6653 \n",
      "Prediction:\n",
      " [[ 151.45576477]\n",
      " [ 187.69163513]\n",
      " [ 181.95057678]\n",
      " [ 198.56752014]\n",
      " [ 144.29502869]\n",
      " [ 107.83660126]\n",
      " [ 147.78050232]\n",
      " [ 108.15636444]\n",
      " [ 177.30737305]\n",
      " [ 165.75004578]]\n",
      "1200 Cost:  12.5802 \n",
      "Prediction:\n",
      " [[ 151.46549988]\n",
      " [ 187.67832947]\n",
      " [ 181.94999695]\n",
      " [ 198.57121277]\n",
      " [ 144.27510071]\n",
      " [ 107.82565308]\n",
      " [ 147.79669189]\n",
      " [ 108.18789673]\n",
      " [ 177.29441833]\n",
      " [ 165.74537659]]\n",
      "1220 Cost:  12.4959 \n",
      "Prediction:\n",
      " [[ 151.47521973]\n",
      " [ 187.66513062]\n",
      " [ 181.94946289]\n",
      " [ 198.57492065]\n",
      " [ 144.25531006]\n",
      " [ 107.81477356]\n",
      " [ 147.81283569]\n",
      " [ 108.21929169]\n",
      " [ 177.28155518]\n",
      " [ 165.74072266]]\n",
      "1240 Cost:  12.4125 \n",
      "Prediction:\n",
      " [[ 151.48486328]\n",
      " [ 187.65196228]\n",
      " [ 181.94892883]\n",
      " [ 198.57859802]\n",
      " [ 144.2355957 ]\n",
      " [ 107.80393219]\n",
      " [ 147.82888794]\n",
      " [ 108.25051117]\n",
      " [ 177.26875305]\n",
      " [ 165.73606873]]\n",
      "1260 Cost:  12.33 \n",
      "Prediction:\n",
      " [[ 151.49449158]\n",
      " [ 187.6388855 ]\n",
      " [ 181.94837952]\n",
      " [ 198.58224487]\n",
      " [ 144.21598816]\n",
      " [ 107.7931366 ]\n",
      " [ 147.84486389]\n",
      " [ 108.28158569]\n",
      " [ 177.2559967 ]\n",
      " [ 165.73144531]]\n",
      "1280 Cost:  12.2483 \n",
      "Prediction:\n",
      " [[ 151.50404358]\n",
      " [ 187.62586975]\n",
      " [ 181.94784546]\n",
      " [ 198.58587646]\n",
      " [ 144.19647217]\n",
      " [ 107.78240967]\n",
      " [ 147.86074829]\n",
      " [ 108.3125    ]\n",
      " [ 177.24330139]\n",
      " [ 165.72683716]]\n",
      "1300 Cost:  12.1673 \n",
      "Prediction:\n",
      " [[ 151.51356506]\n",
      " [ 187.61291504]\n",
      " [ 181.94729614]\n",
      " [ 198.5894928 ]\n",
      " [ 144.17704773]\n",
      " [ 107.77172089]\n",
      " [ 147.8765564 ]\n",
      " [ 108.34326172]\n",
      " [ 177.23065186]\n",
      " [ 165.72224426]]\n",
      "1320 Cost:  12.0873 \n",
      "Prediction:\n",
      " [[ 151.52304077]\n",
      " [ 187.60002136]\n",
      " [ 181.94676208]\n",
      " [ 198.59309387]\n",
      " [ 144.1577301 ]\n",
      " [ 107.76107788]\n",
      " [ 147.89228821]\n",
      " [ 108.37385559]\n",
      " [ 177.21809387]\n",
      " [ 165.71766663]]\n",
      "1340 Cost:  12.008 \n",
      "Prediction:\n",
      " [[ 151.5324707 ]\n",
      " [ 187.58718872]\n",
      " [ 181.94624329]\n",
      " [ 198.59669495]\n",
      " [ 144.13848877]\n",
      " [ 107.75049591]\n",
      " [ 147.90794373]\n",
      " [ 108.4043045 ]\n",
      " [ 177.20556641]\n",
      " [ 165.71310425]]\n",
      "1360 Cost:  11.9295 \n",
      "Prediction:\n",
      " [[ 151.54185486]\n",
      " [ 187.57441711]\n",
      " [ 181.94570923]\n",
      " [ 198.60025024]\n",
      " [ 144.11935425]\n",
      " [ 107.73995972]\n",
      " [ 147.92350769]\n",
      " [ 108.43460846]\n",
      " [ 177.19309998]\n",
      " [ 165.70855713]]\n",
      "1380 Cost:  11.8519 \n",
      "Prediction:\n",
      " [[ 151.5512085 ]\n",
      " [ 187.56170654]\n",
      " [ 181.94519043]\n",
      " [ 198.60379028]\n",
      " [ 144.10032654]\n",
      " [ 107.72946167]\n",
      " [ 147.93899536]\n",
      " [ 108.46474457]\n",
      " [ 177.18070984]\n",
      " [ 165.70402527]]\n",
      "1400 Cost:  11.775 \n",
      "Prediction:\n",
      " [[ 151.5605011 ]\n",
      " [ 187.54905701]\n",
      " [ 181.94465637]\n",
      " [ 198.60731506]\n",
      " [ 144.08135986]\n",
      " [ 107.71903229]\n",
      " [ 147.95440674]\n",
      " [ 108.49472809]\n",
      " [ 177.16835022]\n",
      " [ 165.69949341]]\n",
      "1420 Cost:  11.6989 \n",
      "Prediction:\n",
      " [[ 151.56976318]\n",
      " [ 187.53646851]\n",
      " [ 181.94415283]\n",
      " [ 198.61083984]\n",
      " [ 144.06251526]\n",
      " [ 107.70863342]\n",
      " [ 147.96975708]\n",
      " [ 108.52456665]\n",
      " [ 177.15606689]\n",
      " [ 165.69500732]]\n",
      "1440 Cost:  11.6236 \n",
      "Prediction:\n",
      " [[ 151.57896423]\n",
      " [ 187.5239563 ]\n",
      " [ 181.94364929]\n",
      " [ 198.61433411]\n",
      " [ 144.04376221]\n",
      " [ 107.69828796]\n",
      " [ 147.98500061]\n",
      " [ 108.55425262]\n",
      " [ 177.1438446 ]\n",
      " [ 165.69052124]]\n",
      "1460 Cost:  11.549 \n",
      "Prediction:\n",
      " [[ 151.58815002]\n",
      " [ 187.51148987]\n",
      " [ 181.94314575]\n",
      " [ 198.61781311]\n",
      " [ 144.02508545]\n",
      " [ 107.68798828]\n",
      " [ 148.00019836]\n",
      " [ 108.58380127]\n",
      " [ 177.13166809]\n",
      " [ 165.68605042]]\n",
      "1480 Cost:  11.4752 \n",
      "Prediction:\n",
      " [[ 151.59725952]\n",
      " [ 187.49908447]\n",
      " [ 181.94262695]\n",
      " [ 198.62124634]\n",
      " [ 144.0065155 ]\n",
      " [ 107.67774963]\n",
      " [ 148.01530457]\n",
      " [ 108.6131897 ]\n",
      " [ 177.11955261]\n",
      " [ 165.68159485]]\n",
      "1500 Cost:  11.4022 \n",
      "Prediction:\n",
      " [[ 151.60635376]\n",
      " [ 187.48675537]\n",
      " [ 181.94213867]\n",
      " [ 198.62469482]\n",
      " [ 143.98800659]\n",
      " [ 107.66754913]\n",
      " [ 148.03033447]\n",
      " [ 108.64243317]\n",
      " [ 177.10749817]\n",
      " [ 165.67715454]]\n",
      "1520 Cost:  11.3299 \n",
      "Prediction:\n",
      " [[ 151.61538696]\n",
      " [ 187.47447205]\n",
      " [ 181.94165039]\n",
      " [ 198.62812805]\n",
      " [ 143.96961975]\n",
      " [ 107.65739441]\n",
      " [ 148.04528809]\n",
      " [ 108.67152405]\n",
      " [ 177.09550476]\n",
      " [ 165.67272949]]\n",
      "1540 Cost:  11.2582 \n",
      "Prediction:\n",
      " [[ 151.62437439]\n",
      " [ 187.4622345 ]\n",
      " [ 181.94114685]\n",
      " [ 198.6315155 ]\n",
      " [ 143.9513092 ]\n",
      " [ 107.64728546]\n",
      " [ 148.06015015]\n",
      " [ 108.70046997]\n",
      " [ 177.08354187]\n",
      " [ 165.66830444]]\n",
      "1560 Cost:  11.1874 \n",
      "Prediction:\n",
      " [[ 151.6333313 ]\n",
      " [ 187.45010376]\n",
      " [ 181.94067383]\n",
      " [ 198.63491821]\n",
      " [ 143.93310547]\n",
      " [ 107.63722992]\n",
      " [ 148.07496643]\n",
      " [ 108.72927856]\n",
      " [ 177.07165527]\n",
      " [ 165.66392517]]\n",
      "1580 Cost:  11.1173 \n",
      "Prediction:\n",
      " [[ 151.64225769]\n",
      " [ 187.43800354]\n",
      " [ 181.94018555]\n",
      " [ 198.63829041]\n",
      " [ 143.91499329]\n",
      " [ 107.62722015]\n",
      " [ 148.08970642]\n",
      " [ 108.75792694]\n",
      " [ 177.05982971]\n",
      " [ 165.65956116]]\n",
      "1600 Cost:  11.0479 \n",
      "Prediction:\n",
      " [[ 151.65112305]\n",
      " [ 187.4259491 ]\n",
      " [ 181.93968201]\n",
      " [ 198.64164734]\n",
      " [ 143.8969574 ]\n",
      " [ 107.61724854]\n",
      " [ 148.10435486]\n",
      " [ 108.78643799]\n",
      " [ 177.04804993]\n",
      " [ 165.65519714]]\n",
      "1620 Cost:  10.9792 \n",
      "Prediction:\n",
      " [[ 151.65994263]\n",
      " [ 187.41397095]\n",
      " [ 181.93920898]\n",
      " [ 198.64498901]\n",
      " [ 143.87901306]\n",
      " [ 107.60733795]\n",
      " [ 148.118927  ]\n",
      " [ 108.81481171]\n",
      " [ 177.03631592]\n",
      " [ 165.65084839]]\n",
      "1640 Cost:  10.9112 \n",
      "Prediction:\n",
      " [[ 151.66873169]\n",
      " [ 187.40203857]\n",
      " [ 181.93875122]\n",
      " [ 198.64831543]\n",
      " [ 143.86116028]\n",
      " [ 107.59747314]\n",
      " [ 148.13343811]\n",
      " [ 108.8430481 ]\n",
      " [ 177.0246582 ]\n",
      " [ 165.64651489]]\n",
      "1660 Cost:  10.8438 \n",
      "Prediction:\n",
      " [[ 151.67747498]\n",
      " [ 187.3901825 ]\n",
      " [ 181.9382782 ]\n",
      " [ 198.65159607]\n",
      " [ 143.84339905]\n",
      " [ 107.58763885]\n",
      " [ 148.14787292]\n",
      " [ 108.87112427]\n",
      " [ 177.01304626]\n",
      " [ 165.64219666]]\n",
      "1680 Cost:  10.7772 \n",
      "Prediction:\n",
      " [[ 151.68617249]\n",
      " [ 187.37837219]\n",
      " [ 181.93778992]\n",
      " [ 198.65489197]\n",
      " [ 143.82569885]\n",
      " [ 107.5778656 ]\n",
      " [ 148.16223145]\n",
      " [ 108.89906311]\n",
      " [ 177.0014801 ]\n",
      " [ 165.63789368]]\n",
      "1700 Cost:  10.7113 \n",
      "Prediction:\n",
      " [[ 151.69483948]\n",
      " [ 187.36662292]\n",
      " [ 181.93734741]\n",
      " [ 198.65817261]\n",
      " [ 143.80812073]\n",
      " [ 107.56813049]\n",
      " [ 148.17652893]\n",
      " [ 108.92687225]\n",
      " [ 176.98997498]\n",
      " [ 165.63362122]]\n",
      "1720 Cost:  10.646 \n",
      "Prediction:\n",
      " [[ 151.70346069]\n",
      " [ 187.35493469]\n",
      " [ 181.93687439]\n",
      " [ 198.66143799]\n",
      " [ 143.79060364]\n",
      " [ 107.55844879]\n",
      " [ 148.19073486]\n",
      " [ 108.95453644]\n",
      " [ 176.97851562]\n",
      " [ 165.62934875]]\n",
      "1740 Cost:  10.5813 \n",
      "Prediction:\n",
      " [[ 151.71205139]\n",
      " [ 187.34329224]\n",
      " [ 181.93641663]\n",
      " [ 198.66465759]\n",
      " [ 143.77319336]\n",
      " [ 107.54880524]\n",
      " [ 148.20487976]\n",
      " [ 108.98204803]\n",
      " [ 176.96711731]\n",
      " [ 165.62507629]]\n",
      "1760 Cost:  10.5174 \n",
      "Prediction:\n",
      " [[ 151.72059631]\n",
      " [ 187.33171082]\n",
      " [ 181.93595886]\n",
      " [ 198.66789246]\n",
      " [ 143.75585938]\n",
      " [ 107.53920746]\n",
      " [ 148.21896362]\n",
      " [ 109.00944519]\n",
      " [ 176.95578003]\n",
      " [ 165.62084961]]\n",
      "1780 Cost:  10.4541 \n",
      "Prediction:\n",
      " [[ 151.7290802 ]\n",
      " [ 187.32019043]\n",
      " [ 181.93551636]\n",
      " [ 198.6710968 ]\n",
      " [ 143.73861694]\n",
      " [ 107.52965546]\n",
      " [ 148.23295593]\n",
      " [ 109.03668213]\n",
      " [ 176.94448853]\n",
      " [ 165.61663818]]\n",
      "1800 Cost:  10.3914 \n",
      "Prediction:\n",
      " [[ 151.73754883]\n",
      " [ 187.30873108]\n",
      " [ 181.93505859]\n",
      " [ 198.67428589]\n",
      " [ 143.72145081]\n",
      " [ 107.52014923]\n",
      " [ 148.24688721]\n",
      " [ 109.063797  ]\n",
      " [ 176.93325806]\n",
      " [ 165.61242676]]\n",
      "1820 Cost:  10.3294 \n",
      "Prediction:\n",
      " [[ 151.74595642]\n",
      " [ 187.2973175 ]\n",
      " [ 181.93461609]\n",
      " [ 198.67744446]\n",
      " [ 143.70436096]\n",
      " [ 107.51068115]\n",
      " [ 148.26075745]\n",
      " [ 109.09075928]\n",
      " [ 176.92207336]\n",
      " [ 165.60823059]]\n",
      "1840 Cost:  10.268 \n",
      "Prediction:\n",
      " [[ 151.7543335 ]\n",
      " [ 187.28596497]\n",
      " [ 181.93415833]\n",
      " [ 198.68061829]\n",
      " [ 143.68737793]\n",
      " [ 107.50126648]\n",
      " [ 148.27455139]\n",
      " [ 109.11759949]\n",
      " [ 176.91093445]\n",
      " [ 165.60404968]]\n",
      "1860 Cost:  10.2072 \n",
      "Prediction:\n",
      " [[ 151.76266479]\n",
      " [ 187.2746582 ]\n",
      " [ 181.93371582]\n",
      " [ 198.6837616 ]\n",
      " [ 143.67047119]\n",
      " [ 107.49188995]\n",
      " [ 148.28826904]\n",
      " [ 109.14431   ]\n",
      " [ 176.89985657]\n",
      " [ 165.59988403]]\n",
      "1880 Cost:  10.1471 \n",
      "Prediction:\n",
      " [[ 151.77096558]\n",
      " [ 187.26342773]\n",
      " [ 181.93328857]\n",
      " [ 198.68688965]\n",
      " [ 143.65362549]\n",
      " [ 107.4825592 ]\n",
      " [ 148.3019104 ]\n",
      " [ 109.17086792]\n",
      " [ 176.88882446]\n",
      " [ 165.59573364]]\n",
      "1900 Cost:  10.0875 \n",
      "Prediction:\n",
      " [[ 151.77922058]\n",
      " [ 187.25222778]\n",
      " [ 181.93283081]\n",
      " [ 198.68997192]\n",
      " [ 143.63687134]\n",
      " [ 107.47327423]\n",
      " [ 148.31549072]\n",
      " [ 109.19730377]\n",
      " [ 176.87783813]\n",
      " [ 165.59158325]]\n",
      "1920 Cost:  10.0286 \n",
      "Prediction:\n",
      " [[ 151.78746033]\n",
      " [ 187.24110413]\n",
      " [ 181.93241882]\n",
      " [ 198.69309998]\n",
      " [ 143.620224  ]\n",
      " [ 107.46403503]\n",
      " [ 148.32902527]\n",
      " [ 109.22360992]\n",
      " [ 176.8669281 ]\n",
      " [ 165.58747864]]\n",
      "1940 Cost:  9.97026 \n",
      "Prediction:\n",
      " [[ 151.79562378]\n",
      " [ 187.23001099]\n",
      " [ 181.93199158]\n",
      " [ 198.69616699]\n",
      " [ 143.6036377 ]\n",
      " [ 107.45484161]\n",
      " [ 148.34246826]\n",
      " [ 109.24977875]\n",
      " [ 176.85603333]\n",
      " [ 165.58337402]]\n",
      "1960 Cost:  9.91254 \n",
      "Prediction:\n",
      " [[ 151.80378723]\n",
      " [ 187.21899414]\n",
      " [ 181.93157959]\n",
      " [ 198.69924927]\n",
      " [ 143.5871582 ]\n",
      " [ 107.44567871]\n",
      " [ 148.35586548]\n",
      " [ 109.27581787]\n",
      " [ 176.84521484]\n",
      " [ 165.57929993]]\n",
      "1980 Cost:  9.85541 \n",
      "Prediction:\n",
      " [[ 151.81188965]\n",
      " [ 187.20803833]\n",
      " [ 181.93115234]\n",
      " [ 198.70228577]\n",
      " [ 143.57073975]\n",
      " [ 107.43656921]\n",
      " [ 148.36917114]\n",
      " [ 109.30171967]\n",
      " [ 176.8344574 ]\n",
      " [ 165.57522583]]\n",
      "2000 Cost:  9.79883 \n",
      "Prediction:\n",
      " [[ 151.81996155]\n",
      " [ 187.19709778]\n",
      " [ 181.9307251 ]\n",
      " [ 198.70532227]\n",
      " [ 143.55439758]\n",
      " [ 107.4274826 ]\n",
      " [ 148.38240051]\n",
      " [ 109.32748413]\n",
      " [ 176.82374573]\n",
      " [ 165.57116699]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Coordinator' object has no attribute 'request'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-44f0a628038b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cost: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\nPrediction:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhy_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Coordinator' object has no attribute 'request'"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer(\n",
    "    ['data-01-test-score.csv'], shuffle=False, name='filename_queue')\n",
    "\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "record_defaults=[[0.],[0.],[0.],[0.]]\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "train_x_batch, train_y_batch=\\\n",
    "    tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y=tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X,W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict={X:x_batch,Y:y_batch})\n",
    "    if step%20 ==0:\n",
    "        print(step, 'Cost: ',cost_val,'\\nPrediction:\\n',hy_val)\n",
    "coord.request.stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 로지스틱 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.0706\n",
      "20 4.42305\n",
      "40 2.80961\n",
      "60 1.40735\n",
      "80 0.740663\n",
      "100 0.597842\n",
      "120 0.568736\n",
      "140 0.560687\n",
      "160 0.557043\n",
      "180 0.554421\n",
      "200 0.552063\n",
      "220 0.54979\n",
      "240 0.547555\n",
      "260 0.545346\n",
      "280 0.54316\n",
      "300 0.540996\n",
      "320 0.538851\n",
      "340 0.536727\n",
      "360 0.53462\n",
      "380 0.532532\n",
      "400 0.530461\n",
      "420 0.528406\n",
      "440 0.526368\n",
      "460 0.524345\n",
      "480 0.522337\n",
      "500 0.520344\n",
      "520 0.518366\n",
      "540 0.516401\n",
      "560 0.514449\n",
      "580 0.512511\n",
      "600 0.510586\n",
      "620 0.508673\n",
      "640 0.506772\n",
      "660 0.504883\n",
      "680 0.503006\n",
      "700 0.50114\n",
      "720 0.499285\n",
      "740 0.497442\n",
      "760 0.495609\n",
      "780 0.493787\n",
      "800 0.491975\n",
      "820 0.490173\n",
      "840 0.488382\n",
      "860 0.4866\n",
      "880 0.484828\n",
      "900 0.483066\n",
      "920 0.481314\n",
      "940 0.479571\n",
      "960 0.477837\n",
      "980 0.476112\n",
      "1000 0.474397\n",
      "1020 0.47269\n",
      "1040 0.470992\n",
      "1060 0.469303\n",
      "1080 0.467623\n",
      "1100 0.465952\n",
      "1120 0.464289\n",
      "1140 0.462634\n",
      "1160 0.460988\n",
      "1180 0.459351\n",
      "1200 0.457721\n",
      "1220 0.4561\n",
      "1240 0.454487\n",
      "1260 0.452882\n",
      "1280 0.451285\n",
      "1300 0.449696\n",
      "1320 0.448115\n",
      "1340 0.446542\n",
      "1360 0.444976\n",
      "1380 0.443419\n",
      "1400 0.441869\n",
      "1420 0.440327\n",
      "1440 0.438792\n",
      "1460 0.437265\n",
      "1480 0.435746\n",
      "1500 0.434234\n",
      "1520 0.432729\n",
      "1540 0.431232\n",
      "1560 0.429743\n",
      "1580 0.428261\n",
      "1600 0.426786\n",
      "1620 0.425318\n",
      "1640 0.423858\n",
      "1660 0.422405\n",
      "1680 0.420959\n",
      "1700 0.41952\n",
      "1720 0.418088\n",
      "1740 0.416663\n",
      "1760 0.415246\n",
      "1780 0.413835\n",
      "1800 0.412431\n",
      "1820 0.411035\n",
      "1840 0.409645\n",
      "1860 0.408262\n",
      "1880 0.406886\n",
      "1900 0.405516\n",
      "1920 0.404154\n",
      "1940 0.402798\n",
      "1960 0.401449\n",
      "1980 0.400106\n",
      "2000 0.398771\n",
      "2020 0.397441\n",
      "2040 0.396119\n",
      "2060 0.394803\n",
      "2080 0.393493\n",
      "2100 0.39219\n",
      "2120 0.390894\n",
      "2140 0.389604\n",
      "2160 0.38832\n",
      "2180 0.387043\n",
      "2200 0.385772\n",
      "2220 0.384507\n",
      "2240 0.383249\n",
      "2260 0.381996\n",
      "2280 0.380751\n",
      "2300 0.379511\n",
      "2320 0.378277\n",
      "2340 0.37705\n",
      "2360 0.375829\n",
      "2380 0.374613\n",
      "2400 0.373404\n",
      "2420 0.372201\n",
      "2440 0.371004\n",
      "2460 0.369813\n",
      "2480 0.368628\n",
      "2500 0.367448\n",
      "2520 0.366275\n",
      "2540 0.365107\n",
      "2560 0.363945\n",
      "2580 0.362789\n",
      "2600 0.361639\n",
      "2620 0.360495\n",
      "2640 0.359356\n",
      "2660 0.358223\n",
      "2680 0.357095\n",
      "2700 0.355973\n",
      "2720 0.354857\n",
      "2740 0.353746\n",
      "2760 0.352641\n",
      "2780 0.351541\n",
      "2800 0.350447\n",
      "2820 0.349358\n",
      "2840 0.348275\n",
      "2860 0.347197\n",
      "2880 0.346124\n",
      "2900 0.345057\n",
      "2920 0.343995\n",
      "2940 0.342938\n",
      "2960 0.341887\n",
      "2980 0.340841\n",
      "3000 0.3398\n",
      "3020 0.338764\n",
      "3040 0.337733\n",
      "3060 0.336707\n",
      "3080 0.335687\n",
      "3100 0.334671\n",
      "3120 0.333661\n",
      "3140 0.332656\n",
      "3160 0.331655\n",
      "3180 0.33066\n",
      "3200 0.329669\n",
      "3220 0.328683\n",
      "3240 0.327702\n",
      "3260 0.326726\n",
      "3280 0.325755\n",
      "3300 0.324789\n",
      "3320 0.323827\n",
      "3340 0.32287\n",
      "3360 0.321918\n",
      "3380 0.320971\n",
      "3400 0.320028\n",
      "3420 0.31909\n",
      "3440 0.318156\n",
      "3460 0.317227\n",
      "3480 0.316303\n",
      "3500 0.315383\n",
      "3520 0.314467\n",
      "3540 0.313557\n",
      "3560 0.31265\n",
      "3580 0.311748\n",
      "3600 0.31085\n",
      "3620 0.309957\n",
      "3640 0.309068\n",
      "3660 0.308184\n",
      "3680 0.307304\n",
      "3700 0.306428\n",
      "3720 0.305556\n",
      "3740 0.304688\n",
      "3760 0.303825\n",
      "3780 0.302966\n",
      "3800 0.302111\n",
      "3820 0.30126\n",
      "3840 0.300414\n",
      "3860 0.299571\n",
      "3880 0.298733\n",
      "3900 0.297898\n",
      "3920 0.297068\n",
      "3940 0.296241\n",
      "3960 0.295419\n",
      "3980 0.2946\n",
      "4000 0.293786\n",
      "4020 0.292975\n",
      "4040 0.292168\n",
      "4060 0.291366\n",
      "4080 0.290566\n",
      "4100 0.289771\n",
      "4120 0.28898\n",
      "4140 0.288192\n",
      "4160 0.287408\n",
      "4180 0.286628\n",
      "4200 0.285852\n",
      "4220 0.285079\n",
      "4240 0.28431\n",
      "4260 0.283544\n",
      "4280 0.282782\n",
      "4300 0.282024\n",
      "4320 0.281269\n",
      "4340 0.280518\n",
      "4360 0.279771\n",
      "4380 0.279027\n",
      "4400 0.278286\n",
      "4420 0.277549\n",
      "4440 0.276816\n",
      "4460 0.276085\n",
      "4480 0.275359\n",
      "4500 0.274635\n",
      "4520 0.273915\n",
      "4540 0.273199\n",
      "4560 0.272486\n",
      "4580 0.271776\n",
      "4600 0.271069\n",
      "4620 0.270366\n",
      "4640 0.269666\n",
      "4660 0.268969\n",
      "4680 0.268275\n",
      "4700 0.267585\n",
      "4720 0.266898\n",
      "4740 0.266213\n",
      "4760 0.265533\n",
      "4780 0.264855\n",
      "4800 0.26418\n",
      "4820 0.263509\n",
      "4840 0.26284\n",
      "4860 0.262175\n",
      "4880 0.261512\n",
      "4900 0.260853\n",
      "4920 0.260197\n",
      "4940 0.259543\n",
      "4960 0.258893\n",
      "4980 0.258246\n",
      "5000 0.257601\n",
      "5020 0.25696\n",
      "5040 0.256321\n",
      "5060 0.255685\n",
      "5080 0.255052\n",
      "5100 0.254422\n",
      "5120 0.253795\n",
      "5140 0.253171\n",
      "5160 0.252549\n",
      "5180 0.25193\n",
      "5200 0.251314\n",
      "5220 0.250701\n",
      "5240 0.250091\n",
      "5260 0.249483\n",
      "5280 0.248878\n",
      "5300 0.248275\n",
      "5320 0.247676\n",
      "5340 0.247079\n",
      "5360 0.246484\n",
      "5380 0.245892\n",
      "5400 0.245303\n",
      "5420 0.244717\n",
      "5440 0.244133\n",
      "5460 0.243551\n",
      "5480 0.242972\n",
      "5500 0.242396\n",
      "5520 0.241822\n",
      "5540 0.241251\n",
      "5560 0.240682\n",
      "5580 0.240116\n",
      "5600 0.239552\n",
      "5620 0.238991\n",
      "5640 0.238432\n",
      "5660 0.237875\n",
      "5680 0.237321\n",
      "5700 0.23677\n",
      "5720 0.23622\n",
      "5740 0.235674\n",
      "5760 0.235129\n",
      "5780 0.234587\n",
      "5800 0.234047\n",
      "5820 0.233509\n",
      "5840 0.232974\n",
      "5860 0.232441\n",
      "5880 0.23191\n",
      "5900 0.231382\n",
      "5920 0.230856\n",
      "5940 0.230332\n",
      "5960 0.22981\n",
      "5980 0.229291\n",
      "6000 0.228773\n",
      "6020 0.228258\n",
      "6040 0.227745\n",
      "6060 0.227234\n",
      "6080 0.226726\n",
      "6100 0.226219\n",
      "6120 0.225715\n",
      "6140 0.225213\n",
      "6160 0.224713\n",
      "6180 0.224215\n",
      "6200 0.223719\n",
      "6220 0.223225\n",
      "6240 0.222733\n",
      "6260 0.222243\n",
      "6280 0.221755\n",
      "6300 0.22127\n",
      "6320 0.220786\n",
      "6340 0.220304\n",
      "6360 0.219824\n",
      "6380 0.219347\n",
      "6400 0.218871\n",
      "6420 0.218397\n",
      "6440 0.217925\n",
      "6460 0.217455\n",
      "6480 0.216987\n",
      "6500 0.216521\n",
      "6520 0.216057\n",
      "6540 0.215595\n",
      "6560 0.215134\n",
      "6580 0.214676\n",
      "6600 0.214219\n",
      "6620 0.213764\n",
      "6640 0.213311\n",
      "6660 0.21286\n",
      "6680 0.21241\n",
      "6700 0.211963\n",
      "6720 0.211517\n",
      "6740 0.211073\n",
      "6760 0.210631\n",
      "6780 0.21019\n",
      "6800 0.209752\n",
      "6820 0.209315\n",
      "6840 0.20888\n",
      "6860 0.208446\n",
      "6880 0.208015\n",
      "6900 0.207585\n",
      "6920 0.207156\n",
      "6940 0.20673\n",
      "6960 0.206305\n",
      "6980 0.205881\n",
      "7000 0.20546\n",
      "7020 0.20504\n",
      "7040 0.204622\n",
      "7060 0.204205\n",
      "7080 0.20379\n",
      "7100 0.203377\n",
      "7120 0.202965\n",
      "7140 0.202555\n",
      "7160 0.202146\n",
      "7180 0.201739\n",
      "7200 0.201334\n",
      "7220 0.20093\n",
      "7240 0.200528\n",
      "7260 0.200127\n",
      "7280 0.199728\n",
      "7300 0.19933\n",
      "7320 0.198934\n",
      "7340 0.198539\n",
      "7360 0.198146\n",
      "7380 0.197755\n",
      "7400 0.197365\n",
      "7420 0.196976\n",
      "7440 0.196589\n",
      "7460 0.196203\n",
      "7480 0.195819\n",
      "7500 0.195436\n",
      "7520 0.195055\n",
      "7540 0.194675\n",
      "7560 0.194297\n",
      "7580 0.19392\n",
      "7600 0.193544\n",
      "7620 0.19317\n",
      "7640 0.192798\n",
      "7660 0.192426\n",
      "7680 0.192056\n",
      "7700 0.191688\n",
      "7720 0.19132\n",
      "7740 0.190955\n",
      "7760 0.19059\n",
      "7780 0.190227\n",
      "7800 0.189865\n",
      "7820 0.189505\n",
      "7840 0.189146\n",
      "7860 0.188788\n",
      "7880 0.188432\n",
      "7900 0.188076\n",
      "7920 0.187723\n",
      "7940 0.18737\n",
      "7960 0.187019\n",
      "7980 0.186669\n",
      "8000 0.18632\n",
      "8020 0.185973\n",
      "8040 0.185627\n",
      "8060 0.185282\n",
      "8080 0.184938\n",
      "8100 0.184596\n",
      "8120 0.184255\n",
      "8140 0.183915\n",
      "8160 0.183576\n",
      "8180 0.183239\n",
      "8200 0.182902\n",
      "8220 0.182567\n",
      "8240 0.182234\n",
      "8260 0.181901\n",
      "8280 0.181569\n",
      "8300 0.181239\n",
      "8320 0.18091\n",
      "8340 0.180582\n",
      "8360 0.180255\n",
      "8380 0.17993\n",
      "8400 0.179605\n",
      "8420 0.179282\n",
      "8440 0.17896\n",
      "8460 0.178639\n",
      "8480 0.178319\n",
      "8500 0.178\n",
      "8520 0.177683\n",
      "8540 0.177366\n",
      "8560 0.177051\n",
      "8580 0.176737\n",
      "8600 0.176423\n",
      "8620 0.176111\n",
      "8640 0.1758\n",
      "8660 0.17549\n",
      "8680 0.175181\n",
      "8700 0.174874\n",
      "8720 0.174567\n",
      "8740 0.174261\n",
      "8760 0.173957\n",
      "8780 0.173653\n",
      "8800 0.17335\n",
      "8820 0.173049\n",
      "8840 0.172748\n",
      "8860 0.172449\n",
      "8880 0.172151\n",
      "8900 0.171853\n",
      "8920 0.171557\n",
      "8940 0.171262\n",
      "8960 0.170967\n",
      "8980 0.170674\n",
      "9000 0.170382\n",
      "9020 0.17009\n",
      "9040 0.1698\n",
      "9060 0.16951\n",
      "9080 0.169222\n",
      "9100 0.168935\n",
      "9120 0.168648\n",
      "9140 0.168363\n",
      "9160 0.168078\n",
      "9180 0.167794\n",
      "9200 0.167512\n",
      "9220 0.16723\n",
      "9240 0.166949\n",
      "9260 0.166669\n",
      "9280 0.166391\n",
      "9300 0.166113\n",
      "9320 0.165836\n",
      "9340 0.165559\n",
      "9360 0.165284\n",
      "9380 0.16501\n",
      "9400 0.164736\n",
      "9420 0.164464\n",
      "9440 0.164192\n",
      "9460 0.163921\n",
      "9480 0.163652\n",
      "9500 0.163383\n",
      "9520 0.163115\n",
      "9540 0.162847\n",
      "9560 0.162581\n",
      "9580 0.162316\n",
      "9600 0.162051\n",
      "9620 0.161787\n",
      "9640 0.161524\n",
      "9660 0.161262\n",
      "9680 0.161001\n",
      "9700 0.160741\n",
      "9720 0.160481\n",
      "9740 0.160222\n",
      "9760 0.159965\n",
      "9780 0.159708\n",
      "9800 0.159451\n",
      "9820 0.159196\n",
      "9840 0.158941\n",
      "9860 0.158688\n",
      "9880 0.158435\n",
      "9900 0.158182\n",
      "9920 0.157931\n",
      "9940 0.157681\n",
      "9960 0.157431\n",
      "9980 0.157182\n",
      "10000 0.156934\n",
      "\n",
      "Hypothesis:  [[ 0.03392991]\n",
      " [ 0.16295588]\n",
      " [ 0.31978431]\n",
      " [ 0.77464855]\n",
      " [ 0.93525249]\n",
      " [ 0.97872937]] \n",
      "Predicted (Y):  [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "X_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "X = tf.placeholder(tf.float32, shape=[None,2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.div(1., 1.+tf.exp(-(tf.matmul(X,W)+b)))\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:X_data, Y:y_data})\n",
    "        if step%20 ==0:\n",
    "            print (step,cost_val)\n",
    "    \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:X_data, Y:y_data})\n",
    "    print ('\\nHypothesis: ', h, '\\nPredicted (Y): ', c, '\\nAccuracy: ', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스가 3개 이상인 분류문제(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.10347\n",
      "20 1.09854\n",
      "40 1.09553\n",
      "60 1.09312\n",
      "80 1.09118\n",
      "100 1.08959\n",
      "120 1.08825\n",
      "140 1.08711\n",
      "160 1.08612\n",
      "180 1.08526\n",
      "200 1.08449\n",
      "220 1.0838\n",
      "240 1.08318\n",
      "260 1.08261\n",
      "280 1.08208\n",
      "300 1.0816\n",
      "320 1.08114\n",
      "340 1.08072\n",
      "360 1.08031\n",
      "380 1.07993\n",
      "400 1.07957\n",
      "420 1.07922\n",
      "440 1.07888\n",
      "460 1.07856\n",
      "480 1.07824\n",
      "500 1.07794\n",
      "520 1.07764\n",
      "540 1.07735\n",
      "560 1.07706\n",
      "580 1.07677\n",
      "600 1.07649\n",
      "620 1.07622\n",
      "640 1.07594\n",
      "660 1.07567\n",
      "680 1.0754\n",
      "700 1.07513\n",
      "720 1.07486\n",
      "740 1.07459\n",
      "760 1.07432\n",
      "780 1.07405\n",
      "800 1.07378\n",
      "820 1.07351\n",
      "840 1.07324\n",
      "860 1.07297\n",
      "880 1.0727\n",
      "900 1.07243\n",
      "920 1.07216\n",
      "940 1.07189\n",
      "960 1.07162\n",
      "980 1.07135\n",
      "1000 1.07108\n",
      "1020 1.07081\n",
      "1040 1.07055\n",
      "1060 1.07028\n",
      "1080 1.07002\n",
      "1100 1.06976\n",
      "1120 1.0695\n",
      "1140 1.06924\n",
      "1160 1.06899\n",
      "1180 1.06873\n",
      "1200 1.06848\n",
      "1220 1.06823\n",
      "1240 1.06799\n",
      "1260 1.06775\n",
      "1280 1.0675\n",
      "1300 1.06727\n",
      "1320 1.06703\n",
      "1340 1.0668\n",
      "1360 1.06657\n",
      "1380 1.06634\n",
      "1400 1.06612\n",
      "1420 1.0659\n",
      "1440 1.06568\n",
      "1460 1.06547\n",
      "1480 1.06525\n",
      "1500 1.06505\n",
      "1520 1.06484\n",
      "1540 1.06464\n",
      "1560 1.06444\n",
      "1580 1.06424\n",
      "1600 1.06405\n",
      "1620 1.06385\n",
      "1640 1.06366\n",
      "1660 1.06348\n",
      "1680 1.0633\n",
      "1700 1.06311\n",
      "1720 1.06294\n",
      "1740 1.06276\n",
      "1760 1.06259\n",
      "1780 1.06242\n",
      "1800 1.06225\n",
      "1820 1.06208\n",
      "1840 1.06192\n",
      "1860 1.06176\n",
      "1880 1.0616\n",
      "1900 1.06144\n",
      "1920 1.06129\n",
      "1940 1.06114\n",
      "1960 1.06099\n",
      "1980 1.06084\n",
      "2000 1.06069\n",
      "[[ 0.37927246  0.16693935  0.45378819]]\n"
     ]
    }
   ],
   "source": [
    "X_data = [[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],\n",
    "          [1,6,6,6],[1,7,7,7]]\n",
    "y_data = [[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]]\n",
    "\n",
    "X = tf.placeholder('float', [None,4])\n",
    "Y = tf.placeholder('float', [None,3])\n",
    "nb_classes=3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4,nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n",
    "\n",
    "linear = tf.matmul(X,W)+b\n",
    "logit = tf.div(1., 1.+tf.exp(-linear))\n",
    "softmax = tf.div(tf.exp(logit), tf.reduce_sum(tf.exp(logit),1, keep_dims=True))\n",
    "#softmax = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(softmax),1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X:X_data, Y:y_data})\n",
    "        if step%20==0:\n",
    "            print (step, sess.run(cost,feed_dict={X:X_data, Y:y_data}))\n",
    "\n",
    "    a = sess.run(softmax, feed_dict={X:[[1,11,7,9]]})\n",
    "    print (a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### softmax-cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 9.102\tAcc: 0.00%\n",
      "Step:   100\tLoss: 0.609\tAcc: 90.00%\n",
      "Step:   200\tLoss: 0.661\tAcc: 80.00%\n",
      "Step:   300\tLoss: 0.867\tAcc: 70.00%\n",
      "Step:   400\tLoss: 0.316\tAcc: 90.00%\n",
      "Step:   500\tLoss: 0.197\tAcc: 90.00%\n",
      "Step:   600\tLoss: 0.061\tAcc: 100.00%\n",
      "Step:   700\tLoss: 0.040\tAcc: 100.00%\n",
      "Step:   800\tLoss: 0.107\tAcc: 100.00%\n",
      "Step:   900\tLoss: 0.065\tAcc: 100.00%\n",
      "Step:  1000\tLoss: 0.025\tAcc: 100.00%\n",
      "Step:  1100\tLoss: 0.089\tAcc: 100.00%\n",
      "Step:  1200\tLoss: 0.167\tAcc: 100.00%\n",
      "Step:  1300\tLoss: 0.204\tAcc: 100.00%\n",
      "Step:  1400\tLoss: 0.050\tAcc: 100.00%\n",
      "Step:  1500\tLoss: 0.088\tAcc: 100.00%\n",
      "Step:  1600\tLoss: 0.023\tAcc: 100.00%\n",
      "Step:  1700\tLoss: 0.022\tAcc: 100.00%\n",
      "Step:  1800\tLoss: 0.046\tAcc: 100.00%\n",
      "Step:  1900\tLoss: 0.030\tAcc: 100.00%\n",
      "Step:  2000\tLoss: 0.010\tAcc: 100.00%\n",
      "Step:  2100\tLoss: 0.045\tAcc: 100.00%\n",
      "Step:  2200\tLoss: 0.101\tAcc: 100.00%\n",
      "Step:  2300\tLoss: 0.086\tAcc: 100.00%\n",
      "Step:  2400\tLoss: 0.045\tAcc: 100.00%\n",
      "Step:  2500\tLoss: 0.028\tAcc: 100.00%\n",
      "Step:  2600\tLoss: 0.047\tAcc: 100.00%\n",
      "Step:  2700\tLoss: 0.022\tAcc: 100.00%\n",
      "Step:  2800\tLoss: 0.029\tAcc: 100.00%\n",
      "Step:  2900\tLoss: 0.018\tAcc: 100.00%\n",
      "Step:  3000\tLoss: 0.006\tAcc: 100.00%\n",
      "Step:  3100\tLoss: 0.030\tAcc: 100.00%\n",
      "Step:  3200\tLoss: 0.075\tAcc: 100.00%\n",
      "Step:  3300\tLoss: 0.059\tAcc: 100.00%\n",
      "Step:  3400\tLoss: 0.029\tAcc: 100.00%\n",
      "Step:  3500\tLoss: 0.018\tAcc: 100.00%\n",
      "Step:  3600\tLoss: 0.041\tAcc: 100.00%\n",
      "Step:  3700\tLoss: 0.017\tAcc: 100.00%\n",
      "Step:  3800\tLoss: 0.021\tAcc: 100.00%\n",
      "Step:  3900\tLoss: 0.011\tAcc: 100.00%\n",
      "Step:  4000\tLoss: 0.007\tAcc: 100.00%\n",
      "Step:  4100\tLoss: 0.022\tAcc: 100.00%\n",
      "Step:  4200\tLoss: 0.059\tAcc: 100.00%\n",
      "Step:  4300\tLoss: 0.045\tAcc: 100.00%\n",
      "Step:  4400\tLoss: 0.023\tAcc: 100.00%\n",
      "Step:  4500\tLoss: 0.014\tAcc: 100.00%\n",
      "Step:  4600\tLoss: 0.034\tAcc: 100.00%\n",
      "Step:  4700\tLoss: 0.013\tAcc: 100.00%\n",
      "Step:  4800\tLoss: 0.009\tAcc: 100.00%\n",
      "Step:  4900\tLoss: 0.016\tAcc: 100.00%\n",
      "Step:  5000\tLoss: 0.007\tAcc: 100.00%\n",
      "Step:  5100\tLoss: 0.017\tAcc: 100.00%\n",
      "Step:  5200\tLoss: 0.048\tAcc: 100.00%\n",
      "Step:  5300\tLoss: 0.038\tAcc: 100.00%\n",
      "Step:  5400\tLoss: 0.019\tAcc: 100.00%\n",
      "Step:  5500\tLoss: 0.011\tAcc: 100.00%\n",
      "Step:  5600\tLoss: 0.029\tAcc: 100.00%\n",
      "Step:  5700\tLoss: 0.011\tAcc: 100.00%\n",
      "Step:  5800\tLoss: 0.006\tAcc: 100.00%\n",
      "Step:  5900\tLoss: 0.012\tAcc: 100.00%\n",
      "Step:  6000\tLoss: 0.008\tAcc: 100.00%\n",
      "Step:  6100\tLoss: 0.014\tAcc: 100.00%\n",
      "Step:  6200\tLoss: 0.036\tAcc: 100.00%\n",
      "Step:  6300\tLoss: 0.025\tAcc: 100.00%\n",
      "Step:  6400\tLoss: 0.027\tAcc: 100.00%\n",
      "Step:  6500\tLoss: 0.008\tAcc: 100.00%\n",
      "Step:  6600\tLoss: 0.025\tAcc: 100.00%\n",
      "Step:  6700\tLoss: 0.012\tAcc: 100.00%\n",
      "Step:  6800\tLoss: 0.002\tAcc: 100.00%\n",
      "Step:  6900\tLoss: 0.014\tAcc: 100.00%\n",
      "Step:  7000\tLoss: 0.007\tAcc: 100.00%\n",
      "Step:  7100\tLoss: 0.011\tAcc: 100.00%\n",
      "Step:  7200\tLoss: 0.033\tAcc: 100.00%\n",
      "Step:  7300\tLoss: 0.021\tAcc: 100.00%\n",
      "Step:  7400\tLoss: 0.023\tAcc: 100.00%\n",
      "Step:  7500\tLoss: 0.007\tAcc: 100.00%\n",
      "Step:  7600\tLoss: 0.023\tAcc: 100.00%\n",
      "Step:  7700\tLoss: 0.011\tAcc: 100.00%\n",
      "Step:  7800\tLoss: 0.001\tAcc: 100.00%\n",
      "Step:  7900\tLoss: 0.012\tAcc: 100.00%\n",
      "Step:  8000\tLoss: 0.005\tAcc: 100.00%\n",
      "Step:  8100\tLoss: 0.011\tAcc: 100.00%\n",
      "Step:  8200\tLoss: 0.024\tAcc: 100.00%\n",
      "Step:  8300\tLoss: 0.024\tAcc: 100.00%\n",
      "Step:  8400\tLoss: 0.021\tAcc: 100.00%\n",
      "Step:  8500\tLoss: 0.005\tAcc: 100.00%\n",
      "Step:  8600\tLoss: 0.021\tAcc: 100.00%\n",
      "Step:  8700\tLoss: 0.009\tAcc: 100.00%\n",
      "Step:  8800\tLoss: 0.001\tAcc: 100.00%\n",
      "Step:  8900\tLoss: 0.011\tAcc: 100.00%\n",
      "Step:  9000\tLoss: 0.005\tAcc: 100.00%\n",
      "Step:  9100\tLoss: 0.002\tAcc: 100.00%\n",
      "Step:  9200\tLoss: 0.029\tAcc: 100.00%\n",
      "Step:  9300\tLoss: 0.022\tAcc: 100.00%\n",
      "Step:  9400\tLoss: 0.019\tAcc: 100.00%\n",
      "Step:  9500\tLoss: 0.004\tAcc: 100.00%\n",
      "Step:  9600\tLoss: 0.020\tAcc: 100.00%\n",
      "Step:  9700\tLoss: 0.009\tAcc: 100.00%\n",
      "Step:  9800\tLoss: 0.001\tAcc: 100.00%\n",
      "Step:  9900\tLoss: 0.008\tAcc: 100.00%\n",
      "Step: 10000\tLoss: 0.005\tAcc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer(['data-04-zoo.csv'], shuffle=False, name='filename_queue')\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "record_defaults=[[0.]]*17\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "train_x_batch, train_y_batch = tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n",
    "nb_classes=7\n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None,16])\n",
    "Y=tf.placeholder(tf.int32, shape=[None,1])\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y,nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1,nb_classes])\n",
    "\n",
    "W=tf.Variable(tf.random_normal([16,nb_classes]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "logits = tf.matmul(X,W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    for step in range(10001):\n",
    "        x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "        sess.run(optimizer, feed_dict={X:x_batch, Y:y_batch})\n",
    "        if step%100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy],feed_dict={X:x_batch, Y:y_batch})\n",
    "            print (\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step,loss,acc))\n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes=10\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None, 784])\n",
    "Y=tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W=tf.Variable(tf.random_normal([784,nb_classes]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([nb_classes]), name='weight')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X:batch_xs, Y:batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        print ('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "    print (\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solving XOR using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.728957 [[ 0.70837927  0.79463416  0.54132283 -0.71077824 -0.45960951]\n",
      " [ 0.21429512  0.74477041  0.38027644  0.6988312  -0.08455852]]\n",
      "1000 0.691885 [[ 0.68345296  0.87678176  0.50647205 -0.66455334 -0.47320586]\n",
      " [ 0.15238281  0.83145952  0.3380793   0.80737108 -0.12005953]]\n",
      "2000 0.689005 [[ 0.67537409  1.05311847  0.48264721 -0.68307573 -0.47984311]\n",
      " [ 0.11045633  1.0193249   0.30390185  1.00392044 -0.15055659]]\n",
      "3000 0.67422 [[ 0.68622357  1.47936797  0.47764415 -0.85962862 -0.47568229]\n",
      " [ 0.07493007  1.48788834  0.26636669  1.38369858 -0.18629129]]\n",
      "4000 0.538137 [[ 0.91054517  2.56318712  0.72121447 -1.52263665 -0.38303977]\n",
      " [ 0.0101263   2.83775234  0.19809364  2.45912004 -0.22089463]]\n",
      "5000 0.105471 [[  2.47560573e+00   3.82468271e+00   2.16604543e+00  -2.73406839e+00\n",
      "   -5.97262323e-01]\n",
      " [ -3.31407487e-01   4.33776617e+00  -2.84618023e-03   4.15886593e+00\n",
      "   -1.83010757e-01]]\n",
      "6000 0.0281572 [[ 3.02417994  4.27568007  2.58113813 -3.22834039 -0.77402002]\n",
      " [-0.48822713  4.69760513  0.0256803   4.74268532 -0.18803266]]\n",
      "7000 0.0144056 [[ 3.24549985  4.45774698  2.73012066 -3.42723656 -0.85005206]\n",
      " [-0.57373428  4.84275532  0.0534304   4.97556019 -0.19029373]]\n",
      "8000 0.00934692 [[ 3.37692308  4.5639987   2.81310725 -3.54422498 -0.89624459]\n",
      " [-0.6317482   4.92842722  0.07485653  5.11063814 -0.19160759]]\n",
      "9000 0.00681188 [[ 3.46846175  4.63701725  2.86835647 -3.62512207 -0.92893976]\n",
      " [-0.6755116   4.987813    0.09225075  5.20312691 -0.19259654]]\n",
      "10000 0.0053136 [[ 3.53790903  4.69184971  2.9088099  -3.68615079 -0.95407498]\n",
      " [-0.71060747  5.03269768  0.10693906  5.27240276 -0.19343807]]\n",
      "11000 0.00433299 [[ 3.59346795  4.73535633  2.94022965 -3.73475456 -0.9744162 ]\n",
      " [-0.73988646  5.06849957  0.11969206  5.32727289 -0.19419728]]\n",
      "12000 0.00364516 [[ 3.63954997  4.77120066  2.96562958 -3.77491641 -0.99146175]\n",
      " [-0.76499528  5.09812546  0.13099052  5.37241316 -0.19490279]]\n",
      "13000 0.00313796 [[ 3.67878151  4.80154467  2.98677039 -3.80899858 -1.00611031]\n",
      " [-0.7869696   5.1232996   0.14115413  5.41058064 -0.19556956]]\n",
      "14000 0.00274958 [[ 3.71285081  4.82777357  3.00475407 -3.83851051 -1.01893938]\n",
      " [-0.80650228  5.14512634  0.15040547  5.44354153 -0.19620436]]\n",
      "15000 0.00244326 [[ 3.74289632  4.85080576  3.02032161 -3.86447597 -1.03034484]\n",
      " [-0.82408059  5.16434669  0.15890676  5.47245407 -0.19681257]]\n",
      "16000 0.00219587 [[ 3.76972675  4.87129068  3.03398323 -3.88760781 -1.04060268]\n",
      " [-0.84005827  5.18149185  0.16677959  5.49816322 -0.19739962]]\n",
      "17000 0.00199215 [[ 3.79393077  4.88970852  3.04611135 -3.90843606 -1.04992259]\n",
      " [-0.854702    5.19694424  0.17411761  5.52126503 -0.1979658 ]]\n",
      "18000 0.00182166 [[ 3.81595254  4.90642834  3.05698252 -3.92735338 -1.05845714]\n",
      " [-0.8682161   5.21098995  0.18099439  5.54221058 -0.19851367]]\n",
      "19000 0.00167697 [[ 3.83613396  4.92169285  3.06680179 -3.94466424 -1.06632841]\n",
      " [-0.8807624   5.22385168  0.18746866  5.56135798 -0.19904466]]\n",
      "20000 0.00155275 [[ 3.85474539  4.93573904  3.07573581 -3.96059752 -1.07362437]\n",
      " [-0.89246911  5.23570347  0.19358896  5.57895517 -0.19956107]]\n",
      "21000 0.00144505 [[ 3.87200212  4.94872952  3.08391213 -3.97535443 -1.08042967]\n",
      " [-0.90344244  5.24669123  0.19939511  5.59520912 -0.20006326]]\n",
      "22000 0.00135071 [[ 3.88807678  4.9608283   3.09144139 -3.98908138 -1.08680141]\n",
      " [-0.9137668   5.2569232   0.20492019  5.6103425  -0.20055203]]\n",
      "23000 0.00126758 [[ 3.9031136   4.97207689  3.09839439 -4.00189543 -1.09279275]\n",
      " [-0.92351556  5.2665019   0.21019237  5.62445259 -0.20102878]]\n",
      "24000 0.00119369 [[ 3.91723871  4.9826808   3.10485482 -4.0139513  -1.09844279]\n",
      " [-0.93274885  5.27545404  0.21523522  5.63765574 -0.20149162]]\n",
      "25000 0.00112761 [[ 3.93053365  4.99258614  3.11088085 -4.02523232 -1.10379505]\n",
      " [-0.94151962  5.28389931  0.22006981  5.65009499 -0.20194399]]\n",
      "26000 0.00106826 [[ 3.94310331  5.00195122  3.11649752 -4.03592205 -1.10887396]\n",
      " [-0.94986868  5.29188824  0.22471449  5.66180325 -0.2023897 ]]\n",
      "27000 0.00101457 [[ 3.95501256  5.01081753  3.12179208 -4.04604864 -1.11370265]\n",
      " [-0.95783794  5.29948235  0.2291837   5.67291403 -0.20282184]]\n",
      "28000 0.00096585 [[ 3.96631837  5.01923275  3.12674022 -4.05564785 -1.11830354]\n",
      " [-0.96545768  5.3066349   0.23349236  5.68344784 -0.20324425]]\n",
      "29000 0.000921429 [[ 3.97707987  5.02725315  3.13141537 -4.06476164 -1.12270188]\n",
      " [-0.97275871  5.31342793  0.23765205  5.69343185 -0.20366062]]\n",
      "30000 0.000880768 [[ 3.98734784  5.03488255  3.13583684 -4.07343483 -1.12692165]\n",
      " [-0.97976577  5.31998491  0.24167408  5.70296144 -0.20406617]]\n",
      "31000 0.000843436 [[ 3.99716473  5.04209757  3.14002585 -4.08172512 -1.13096166]\n",
      " [-0.98650193  5.3261838   0.24556707  5.71202135 -0.20446724]]\n",
      "32000 0.000809028 [[ 4.00657701  5.04906559  3.14401054 -4.08969212 -1.1348443 ]\n",
      " [-0.99298775  5.33213377  0.24933998  5.72066879 -0.20485467]]\n",
      "33000 0.000777201 [[ 4.01563692  5.05574131  3.14781141 -4.09732151 -1.1385833 ]\n",
      " [-0.99924034  5.33785582  0.25300035  5.72897053 -0.20524047]]\n",
      "34000 0.000747732 [[ 4.0242238   5.06208324  3.15138769 -4.10457039 -1.142187  ]\n",
      " [-1.00527513  5.34333754  0.25655746  5.73699474 -0.205613  ]]\n",
      "35000 0.000720307 [[ 4.03252602  5.06828213  3.15479994 -4.11165953 -1.14566076]\n",
      " [-1.0111059   5.34858274  0.26001418  5.74462414 -0.20598553]]\n",
      "36000 0.000694776 [[ 4.04062557  5.07411528  3.15811086 -4.11833525 -1.14901674]\n",
      " [-1.01675653  5.35376501  0.26337612  5.75197315 -0.20634572]]\n",
      "37000 0.000670932 [[ 4.04825497  5.07983732  3.1612103  -4.12491083 -1.15226185]\n",
      " [-1.0222162   5.35853338  0.2666533   5.75912571 -0.2067026 ]]\n",
      "38000 0.000648594 [[ 4.05576515  5.08531141  3.16424036 -4.13110971 -1.15540123]\n",
      " [-1.02751601  5.36330175  0.26984605  5.76589775 -0.20705675]]\n",
      "39000 0.000627658 [[ 4.06291771  5.09055662  3.16710138 -4.13729525 -1.15844655]\n",
      " [-1.03266764  5.36792946  0.27296123  5.77257347 -0.20739947]]\n",
      "40000 0.000607975 [[ 4.06994057  5.09580183  3.16989064 -4.14301729 -1.16140854]\n",
      " [-1.03765738  5.37222099  0.27600095  5.7788887  -0.20773844]]\n",
      "41000 0.000589486 [[ 4.07661629  5.10063171  3.17251325 -4.14873934 -1.16426957]\n",
      " [-1.04250681  5.37651253  0.27897218  5.78508759 -0.20807151]]\n",
      "42000 0.00057204 [[ 4.08328819  5.10540009  3.17513585 -4.15424919 -1.16705048]\n",
      " [-1.04723334  5.38080406  0.28187239  5.79108429 -0.20840017]]\n",
      "43000 0.000555505 [[ 4.08948708  5.11016846  3.17753148 -4.1594944  -1.16978037]\n",
      " [-1.05182874  5.38470507  0.2847127   5.79680634 -0.208728  ]]\n",
      "44000 0.000539953 [[ 4.09568596  5.11459637  3.17991567 -4.16473961 -1.17240298]\n",
      " [-1.05631161  5.38851976  0.28748944  5.80252838 -0.20904602]]\n",
      "45000 0.000525193 [[ 4.10169601  5.1188879   3.18226218 -4.16976881 -1.1749742 ]\n",
      " [-1.06068075  5.39233446  0.29020831  5.80791664 -0.20935895]]\n",
      "46000 0.000511118 [[ 4.10741806  5.12317944  3.18440795 -4.17453718 -1.1774776 ]\n",
      " [-1.06494236  5.39614916  0.29287532  5.81316185 -0.20967187]]\n",
      "47000 0.000497834 [[ 4.11314011  5.12747097  3.18655372 -4.17930555 -1.17991233]\n",
      " [-1.06910944  5.39973927  0.29548356  5.81840706 -0.20997347]]\n",
      "48000 0.000485131 [[ 4.11869431  5.13135004  3.18869948 -4.18407393 -1.18229651]\n",
      " [-1.07316256  5.40307713  0.29804355  5.82341337 -0.21027149]]\n",
      "49000 0.000473174 [[ 4.12393951  5.13516474  3.19064856 -4.1884222  -1.18462133]\n",
      " [-1.07712054  5.40641499  0.30055362  5.82818174 -0.21056952]]\n",
      "50000 0.000461665 [[ 4.12918472  5.13897943  3.1925559  -4.19271374 -1.18688631]\n",
      " [-1.08099878  5.40975285  0.30301556  5.83295012 -0.21086754]]\n",
      "51000 0.000450662 [[ 4.13442993  5.14279413  3.19446325 -4.19700527 -1.18912625]\n",
      " [-1.08480978  5.41309071  0.30543566  5.83771849 -0.21115655]]\n",
      "52000 0.000440152 [[ 4.13930035  5.14657545  3.1963706  -4.20129681 -1.19127202]\n",
      " [-1.08850527  5.41642857  0.30780888  5.84216166 -0.21143967]]\n",
      "53000 0.000430238 [[ 4.14406872  5.14991331  3.19808006 -4.2053504  -1.19341779]\n",
      " [-1.09213603  5.41933966  0.31014243  5.84645319 -0.21172279]]\n",
      "54000 0.000420592 [[ 4.14883709  5.15325117  3.19974899 -4.2091651  -1.19548368]\n",
      " [-1.0957123   5.42220068  0.31243181  5.85074472 -0.21200062]]\n",
      "55000 0.000411439 [[ 4.15360546  5.15658903  3.20141792 -4.21297979 -1.19751024]\n",
      " [-1.09917629  5.4250617   0.31468728  5.85503626 -0.21227865]]\n",
      "56000 0.000402688 [[ 4.15812445  5.15992689  3.20308685 -4.21679449 -1.1995368 ]\n",
      " [-1.10261476  5.42792273  0.31690243  5.85931015 -0.21254687]]\n",
      "57000 0.000394176 [[ 4.16241598  5.16326475  3.20475578 -4.22060919 -1.20145655]\n",
      " [-1.10595262  5.43078375  0.3190816   5.86312485 -0.21281509]]\n",
      "58000 0.000386126 [[ 4.16670752  5.16660261  3.20622158 -4.22439098 -1.2033639 ]\n",
      " [-1.10924995  5.43364477  0.32122514  5.86693954 -0.21308331]]\n",
      "59000 0.000378314 [[ 4.17099905  5.16951084  3.20765209 -4.22772884 -1.20527124]\n",
      " [-1.1124686   5.43650579  0.32334098  5.87075424 -0.21335153]]\n",
      "60000 0.000370816 [[ 4.17529058  5.17237186  3.2090826  -4.2310667  -1.20711172]\n",
      " [-1.11564124  5.4391818   0.32542154  5.87456894 -0.21360916]]\n",
      "61000 0.000363601 [[ 4.17943382  5.17523289  3.21051311 -4.23440456 -1.20889986]\n",
      " [-1.11874068  5.44156599  0.32746533  5.87838364 -0.21386248]]\n",
      "62000 0.000356669 [[ 4.18324852  5.17809391  3.21194363 -4.23774242 -1.21068799]\n",
      " [-1.12182534  5.44395018  0.32947469  5.88207769 -0.21410796]]\n",
      "63000 0.000349946 [[ 4.18706322  5.18095493  3.21337414 -4.24108028 -1.21247613]\n",
      " [-1.12480557  5.44633436  0.33146787  5.88541555 -0.21435693]]\n",
      "64000 0.000343536 [[ 4.19087791  5.18381596  3.21480346 -4.24441814 -1.21418118]\n",
      " [-1.12777984  5.44871855  0.33343482  5.88875341 -0.21461025]]\n",
      "65000 0.000337365 [[ 4.19469261  5.18667698  3.21600008 -4.24772978 -1.21585011]\n",
      " [-1.13064086  5.45110273  0.33536884  5.89209127 -0.21485808]]\n",
      "66000 0.000331298 [[ 4.19850731  5.189538    3.21719217 -4.2505908  -1.21751904]\n",
      " [-1.13350189  5.45348692  0.33726686  5.89542913 -0.21509644]]\n",
      "67000 0.000325439 [[ 4.20232201  5.19210243  3.21838427 -4.25345182 -1.21918797]\n",
      " [-1.13633728  5.45587111  0.33913687  5.89876699 -0.21532191]]\n",
      "68000 0.00031985 [[ 4.20576429  5.19448662  3.21957636 -4.25631285 -1.22074556]\n",
      " [-1.13907909  5.45825529  0.34098461  5.90210485 -0.21555713]]\n",
      "69000 0.000314498 [[ 4.20910215  5.1968708   3.22076845 -4.25917387 -1.22229528]\n",
      " [-1.14182091  5.46063948  0.34282029  5.90544271 -0.21579555]]\n",
      "70000 0.000309221 [[ 4.21244001  5.19925499  3.22196054 -4.26203489 -1.22384501]\n",
      " [-1.14446855  5.46302366  0.34463388  5.90843582 -0.21603397]]\n",
      "71000 0.000304109 [[ 4.21577787  5.20163918  3.22315264 -4.26489592 -1.22539473]\n",
      " [-1.14709115  5.46503448  0.34641778  5.91129684 -0.21626604]]\n",
      "72000 0.00029916 [[ 4.21911573  5.20402336  3.22434473 -4.26775694 -1.22692668]\n",
      " [-1.14971375  5.46694183  0.34817335  5.91415787 -0.21648955]]\n",
      "73000 0.000294375 [[ 4.22245359  5.20640755  3.22531223 -4.27061796 -1.2283572 ]\n",
      " [-1.15227127  5.46884918  0.34990188  5.91701889 -0.21671307]]\n",
      "74000 0.000289784 [[ 4.22579145  5.20879173  3.22626591 -4.27347898 -1.22978771]\n",
      " [-1.15477467  5.47075653  0.35161027  5.91987991 -0.21693613]]\n",
      "75000 0.000285312 [[ 4.22912931  5.21117592  3.22721958 -4.27600241 -1.23121822]\n",
      " [-1.15727806  5.47266388  0.35330901  5.92274094 -0.21715865]]\n",
      "76000 0.00028093 [[ 4.23235893  5.2135601   3.22817326 -4.27838659 -1.23264873]\n",
      " [-1.15972745  5.47457123  0.35498565  5.92560196 -0.21738216]]\n",
      "77000 0.000276652 [[ 4.23521996  5.21594429  3.22912693 -4.28077078 -1.23407924]\n",
      " [-1.16211164  5.47647858  0.35665435  5.92846298 -0.21760568]]\n",
      "78000 0.000272553 [[ 4.23808098  5.21831036  3.2300806  -4.28315496 -1.23550642]\n",
      " [-1.16449583  5.47838593  0.3582938   5.93132401 -0.21782337]]\n",
      "79000 0.000268573 [[ 4.240942    5.2202177   3.23103428 -4.28553915 -1.23681772]\n",
      " [-1.16688001  5.48029327  0.35990903  5.93418503 -0.21803199]]\n",
      "80000 0.000264713 [[ 4.24380302  5.22212505  3.23198795 -4.28792334 -1.23812902]\n",
      " [-1.1691848   5.48220062  0.3615014   5.93665552 -0.2182406 ]]\n",
      "81000 0.000261001 [[ 4.24666405  5.2240324   3.23294163 -4.29030752 -1.23944032]\n",
      " [-1.17144978  5.48410797  0.36308092  5.93903971 -0.21844922]]\n",
      "82000 0.00025729 [[ 4.24952507  5.22593975  3.2338953  -4.29269171 -1.24075162]\n",
      " [-1.17371476  5.48601532  0.36463487  5.94142389 -0.21865734]]\n",
      "83000 0.000253683 [[ 4.25238609  5.2278471   3.23484898 -4.29507589 -1.24206293]\n",
      " [-1.17597973  5.48792267  0.36618459  5.94380808 -0.21886554]]\n",
      "84000 0.000250254 [[ 4.25524712  5.22975445  3.23580265 -4.29746008 -1.24337423]\n",
      " [-1.17814279  5.48983002  0.3677125   5.94619226 -0.21907416]]\n",
      "85000 0.000246856 [[ 4.25810814  5.2316618   3.23675632 -4.29984426 -1.24465299]\n",
      " [-1.18028855  5.49173737  0.36923242  5.94857645 -0.21928278]]\n",
      "86000 0.000243532 [[ 4.26096916  5.23356915  3.23752189 -4.30222845 -1.24584508]\n",
      " [-1.18243432  5.49364471  0.37072465  5.95096064 -0.219485  ]]\n",
      "87000 0.000240283 [[ 4.26382637  5.23547649  3.23823714 -4.30461264 -1.24703717]\n",
      " [-1.18458009  5.49533463  0.37221113  5.95334482 -0.21967871]]\n",
      "88000 0.000237093 [[ 4.26621056  5.23738384  3.2389524  -4.3067627  -1.24822927]\n",
      " [-1.18667877  5.49676514  0.37367144  5.95572901 -0.21987243]]\n",
      "89000 0.000234067 [[ 4.26859474  5.23929119  3.23966765 -4.30867004 -1.24942136]\n",
      " [-1.18870533  5.49819565  0.37513098  5.95811319 -0.22006615]]\n",
      "90000 0.000231116 [[ 4.27097893  5.24119854  3.24038291 -4.31057739 -1.25061345]\n",
      " [-1.19073188  5.49962616  0.37656176  5.96049738 -0.22025986]]\n",
      "91000 0.000228269 [[ 4.27336311  5.24310589  3.24109817 -4.31248474 -1.25180554]\n",
      " [-1.19275844  5.50105667  0.37798357  5.96288157 -0.22045167]]\n",
      "92000 0.000225467 [[ 4.2757473   5.24501324  3.24181342 -4.31439209 -1.25299764]\n",
      " [-1.194785    5.50248718  0.37938428  5.96526575 -0.22063057]]\n",
      "93000 0.000222725 [[ 4.27813148  5.24692059  3.24252868 -4.31629944 -1.2541225 ]\n",
      " [-1.19680834  5.50391769  0.380766    5.96764994 -0.22080939]]\n",
      "94000 0.000219997 [[ 4.28051567  5.24882793  3.24324393 -4.31820679 -1.25519538]\n",
      " [-1.19871581  5.50534821  0.38213691  5.969666   -0.2209882 ]]\n",
      "95000 0.000217359 [[ 4.28289986  5.25073528  3.24395919 -4.32011414 -1.25626826]\n",
      " [-1.20062315  5.50677872  0.38350782  5.97157335 -0.22116701]]\n",
      "96000 0.000214736 [[ 4.28528404  5.25254011  3.24467444 -4.32202148 -1.25734115]\n",
      " [-1.2025305   5.50820923  0.3848545   5.9734807  -0.22134583]]\n",
      "97000 0.000212157 [[ 4.28766823  5.25397062  3.2453897  -4.32392883 -1.25841403]\n",
      " [-1.20443785  5.50963974  0.3861956   5.97538805 -0.22152464]]\n",
      "98000 0.000209639 [[ 4.29005241  5.25540113  3.24610496 -4.32583618 -1.25948691]\n",
      " [-1.20633638  5.51107025  0.3875367   5.9772954  -0.22171006]]\n",
      "99000 0.000207194 [[ 4.2924366   5.25683165  3.24682021 -4.32774353 -1.2605598 ]\n",
      " [-1.20812452  5.51250076  0.38886037  5.97920275 -0.22189693]]\n",
      "100000 0.000204929 [[ 4.29482079  5.25826216  3.24753547 -4.32965088 -1.26163268]\n",
      " [-1.20991266  5.51393127  0.39017168  5.9811101  -0.22207575]]\n",
      "[array([[  1.12530834e-04],\n",
      "       [  9.99765337e-01],\n",
      "       [  9.99807060e-01],\n",
      "       [  2.79472471e-04]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data=[[0],[1],[1],[0]]\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None,2])\n",
    "Y=tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "'''\n",
    "#two layers\n",
    "W1=tf.Variable(tf.random_uniform([2,2], -1.0, 1.0))\n",
    "W2=tf.Variable(tf.random_uniform([2,1], -1.0, 1.0))\n",
    "b1=tf.Variable(tf.zeros([2]), name='bias1')\n",
    "b2=tf.Variable(tf.zeros([1]), name='bias2')\n",
    "\n",
    "# wide NN\n",
    "W1=tf.Variable(tf.random_uniform([2,10], -1.0, 1.0))\n",
    "W2=tf.Variable(tf.random_uniform([10,1], -1.0, 1.0))\n",
    "b1=tf.Variable(tf.zeros([10]), name='bias1')\n",
    "b2=tf.Variable(tf.zeros([1]), name='bias2')\n",
    "'''\n",
    "#deep NN\n",
    "W1=tf.Variable(tf.random_uniform([2,5], -1.0, 1.0))\n",
    "W2=tf.Variable(tf.random_uniform([5,4], -1.0, 1.0))\n",
    "W3=tf.Variable(tf.random_uniform([4,1], -1.0, 1.0))\n",
    "b1=tf.Variable(tf.zeros([5]), name='bias1')\n",
    "b2=tf.Variable(tf.zeros([4]), name='bias2')\n",
    "b3=tf.Variable(tf.zeros([1]), name='bias3')\n",
    "\n",
    "'''L2=tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2,W2)+b2)\n",
    "'''\n",
    "L2=tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "L3=tf.sigmoid(tf.matmul(L2,W2)+b2)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L3,W3)+b3)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "a=tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in xrange(100001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step%1000==0:\n",
    "            print (step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W1))\n",
    "        \n",
    "    correct_prediction=tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    print (sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction], feed_dict={X:x_data, Y:y_data}))\n",
    "    print ('Accuracy:', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_data=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data=[[0],[1],[1],[0]]\n",
    "x_data=np.array(x_data, dtype=np.float32)\n",
    "y_data=np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None,2], name='X_input')\n",
    "Y=tf.placeholder(tf.float32, [None,1], name='Y_input')\n",
    "\n",
    "#deep NN\n",
    "W1=tf.Variable(tf.random_uniform([2,5], -1.0, 1.0), name='weight1')\n",
    "W2=tf.Variable(tf.random_uniform([5,4], -1.0, 1.0), name='weight2')\n",
    "W3=tf.Variable(tf.random_uniform([4,1], -1.0, 1.0), name='weight3')\n",
    "b1=tf.Variable(tf.zeros([5]), name='bias1')\n",
    "b2=tf.Variable(tf.zeros([4]), name='bias2')\n",
    "b3=tf.Variable(tf.zeros([1]), name='bias3')\n",
    "\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    L2=tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    L3=tf.sigmoid(tf.matmul(L2,W2)+b2)\n",
    "\n",
    "with tf.name_scope(\"layer4\") as scope:    \n",
    "    hypothesis = tf.sigmoid(tf.matmul(L3,W3)+b3)\n",
    "\n",
    "with tf.name_scope(\"costs\") as scope:\n",
    "    cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "    train = optimizer.minimize(cost)\n",
    "    \n",
    "w1_hist=tf.summary.histogram(\"weights1\", W1)\n",
    "w2_hist=tf.summary.histogram(\"weights2\", W2)\n",
    "w3_hist=tf.summary.histogram(\"weights3\", W3)\n",
    "b1_hist=tf.summary.histogram(\"biases1\", b1)\n",
    "b2_hist=tf.summary.histogram(\"biases2\", b2)\n",
    "b3_hist=tf.summary.histogram(\"biases3\", b3)\n",
    "y_hist = tf.summary.histogram(\"y\", Y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"../logs\", sess.graph)\n",
    "    \n",
    "    for step in xrange(100001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step%1000 ==0:\n",
    "            summary, _= sess.run([merged,train], feed_dict={X:x_data, Y:y_data})\n",
    "            writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MNIST with RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.708066634\n",
      "Epoch: 0002 cost = 0.373515352\n",
      "Epoch: 0003 cost = 0.358323813\n",
      "Epoch: 0004 cost = 0.355370102\n",
      "Epoch: 0005 cost = 0.354675549\n",
      "Epoch: 0006 cost = 0.351218239\n",
      "Epoch: 0007 cost = 0.331291266\n",
      "Epoch: 0008 cost = 0.317240680\n",
      "Epoch: 0009 cost = 0.326883071\n",
      "Epoch: 0010 cost = 0.325672709\n",
      "Epoch: 0011 cost = 0.331709048\n",
      "Epoch: 0012 cost = 0.376545494\n",
      "Epoch: 0013 cost = 0.333683162\n",
      "Epoch: 0014 cost = 0.316968780\n",
      "Epoch: 0015 cost = 0.326600642\n",
      "Accuracy:  0.9134\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes=10\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None, 784])\n",
    "Y=tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "#Layer1\n",
    "W1=tf.get_variable(\"W1\", shape=[784,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([512]), name='bias1')\n",
    "L1=tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "L1=tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "#Layer2\n",
    "W2=tf.get_variable(\"W2\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([512]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "L2=tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3=tf.get_variable(\"W2\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([512]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
    "L3=tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4=tf.get_variable(\"W2\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([512]))\n",
    "L4=tf.nn.relu(tf.matmul(L3,W4)+b4)\n",
    "L4=tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "#Layer3\n",
    "W5=tf.get_variable(\"W3\", shape=[512,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "logits=tf.matmul(L4,W5)+b5\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.05).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X:batch_xs, Y:batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        print ('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "    print (\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
